{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "% run MM_Maze_Utils.py\r\n",
                "% run MM_Plot_Utils.py"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "UsageError: Line magic function `%` not found.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Set up"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 149,
            "source": [
                "# Import modules\r\n",
                "import gym\r\n",
                "import enum\r\n",
                "import copy\r\n",
                "import time\r\n",
                "import acme\r\n",
                "import torch\r\n",
                "import base64\r\n",
                "import dm_env\r\n",
                "import IPython\r\n",
                "import imageio\r\n",
                "import warnings\r\n",
                "import itertools\r\n",
                "import collections\r\n",
                "\r\n",
                "import numpy as np\r\n",
                "import pandas as pd\r\n",
                "import seaborn as sns\r\n",
                "import torch.nn as nn\r\n",
                "import torch.optim as optim\r\n",
                "import torch.nn.functional as F\r\n",
                "\r\n",
                "import matplotlib as mpl\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "\r\n",
                "import tensorflow.compat.v2 as tf\r\n",
                "\r\n",
                "from acme import specs\r\n",
                "from acme import wrappers\r\n",
                "from acme.utils import tree_utils\r\n",
                "from acme.utils import loggers\r\n",
                "from torch.autograd import Variable\r\n",
                "from torch.distributions import Categorical\r\n",
                "from typing import Callable, Sequence\r\n",
                "\r\n",
                "tf.enable_v2_behavior()\r\n",
                "warnings.filterwarnings('ignore')\r\n",
                "np.set_printoptions(precision=3, suppress=1)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# @title Figure settings\r\n",
                "import ipywidgets as widgets       # interactive display\r\n",
                "%matplotlib inline\r\n",
                "%config InlineBackend.figure_format = 'retina'\r\n",
                "plt.style.use(\r\n",
                "    \"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\r\n",
                "mpl.rc('image', cmap='Blues')\r\n",
                "\r\n",
                "def map_from_action_to_subplot(a): return (2, 6, 8, 4)[a]\r\n",
                "def map_from_action_to_name(a): return (\"up\", \"right\", \"down\", \"left\")[a]\r\n",
                "\r\n",
                "\r\n",
                "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\r\n",
                "  plt.imshow(values, interpolation=\"nearest\",\r\n",
                "             cmap=colormap, vmin=vmin, vmax=vmax)\r\n",
                "  plt.yticks([])\r\n",
                "  plt.xticks([])\r\n",
                "  plt.colorbar(ticks=[vmin, vmax])\r\n",
                "\r\n",
                "\r\n",
                "def plot_state_value(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(4, 4))\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_action_values(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(8, 8))\r\n",
                "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  dif = vmax - vmin\r\n",
                "  for a in [0, 1, 2, 3]:\r\n",
                "    plt.subplot(3, 3, map_from_action_to_subplot(a))\r\n",
                "\r\n",
                "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\r\n",
                "    action_name = map_from_action_to_name(a)\r\n",
                "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\r\n",
                "\r\n",
                "  plt.subplot(3, 3, 5)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_stats(stats, window=10):\r\n",
                "  plt.figure(figsize=(16, 4))\r\n",
                "  plt.subplot(121)\r\n",
                "  xline = range(0, len(stats.episode_lengths), window)\r\n",
                "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\r\n",
                "  plt.ylabel('Episode Length')\r\n",
                "  plt.xlabel('Episode Count')\r\n",
                "  plt.subplot(122)\r\n",
                "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\r\n",
                "  plt.ylabel('Episode Return')\r\n",
                "  plt.xlabel('Episode Count')\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def map_from_action_to_subplot(a): return (2, 6, 8, 4)[a]\r\n",
                "def map_from_action_to_name(a): return (\"up\", \"right\", \"down\", \"left\")[a]\r\n",
                "\r\n",
                "\r\n",
                "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\r\n",
                "  plt.imshow(values, interpolation=\"nearest\",\r\n",
                "             cmap=colormap, vmin=vmin, vmax=vmax)\r\n",
                "  plt.yticks([])\r\n",
                "  plt.xticks([])\r\n",
                "  plt.colorbar(ticks=[vmin, vmax])\r\n",
                "\r\n",
                "\r\n",
                "def plot_state_value(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(4, 4))\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_action_values(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(8, 8))\r\n",
                "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  dif = vmax - vmin\r\n",
                "  for a in [0, 1, 2, 3]:\r\n",
                "    plt.subplot(3, 3, map_from_action_to_subplot(a))\r\n",
                "\r\n",
                "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\r\n",
                "    action_name = map_from_action_to_name(a)\r\n",
                "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\r\n",
                "\r\n",
                "  plt.subplot(3, 3, 5)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_stats(stats, window=10):\r\n",
                "  plt.figure(figsize=(16, 4))\r\n",
                "  plt.subplot(121)\r\n",
                "  xline = range(0, len(stats.episode_lengths), window)\r\n",
                "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\r\n",
                "  plt.ylabel('Episode Length')\r\n",
                "  plt.xlabel('Episode Count')\r\n",
                "  plt.subplot(122)\r\n",
                "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\r\n",
                "  plt.ylabel('Episode Return')\r\n",
                "  plt.xlabel('Episode Count')\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Grid-world"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 145,
            "source": [
                "# @title Implement GridWorld { form-width: \"30%\" }\r\n",
                "# @markdown *Double-click* to inspect the contents of this cell.\r\n",
                "\r\n",
                "class ObservationType(enum.IntEnum):\r\n",
                "  STATE_INDEX = enum.auto()\r\n",
                "  AGENT_ONEHOT = enum.auto()\r\n",
                "  GRID = enum.auto()\r\n",
                "  AGENT_GOAL_POS = enum.auto()\r\n",
                "\r\n",
                "\r\n",
                "class GridWorld(dm_env.Environment):\r\n",
                "\r\n",
                "  def __init__(self,\r\n",
                "               layout,\r\n",
                "               start_state,\r\n",
                "               goal_state=None,\r\n",
                "               observation_type=ObservationType.STATE_INDEX,\r\n",
                "               discount=0.9,\r\n",
                "               penalty_for_walls=-5,\r\n",
                "               reward_goal=10,\r\n",
                "               max_episode_length=None,\r\n",
                "               randomize_goals=False):\r\n",
                "    \"\"\"Build a grid environment.\r\n",
                "\r\n",
                "    Simple gridworld defined by a map layout, a start and a goal state.\r\n",
                "\r\n",
                "    Layout should be a NxN grid, containing:\r\n",
                "      * 0: empty\r\n",
                "      * -1: wall\r\n",
                "      * Any other positive value: value indicates reward; episode will terminate\r\n",
                "\r\n",
                "    Args:\r\n",
                "      layout: NxN array of numbers, indicating the layout of the environment.\r\n",
                "      start_state: Tuple (y, x) of starting location.\r\n",
                "      goal_state: Optional tuple (y, x) of goal location. Will be randomly\r\n",
                "        sampled once if None.\r\n",
                "      observation_type: Enum observation type to use. One of:\r\n",
                "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\r\n",
                "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\r\n",
                "          agent is and 0 elsewhere.\r\n",
                "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\r\n",
                "          First channel contains walls (1 if wall, 0 otherwise), second the\r\n",
                "          agent position (1 if agent, 0 otherwise) and third goal position\r\n",
                "          (1 if goal, 0 otherwise)\r\n",
                "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\r\n",
                "          (agent_y, agent_x, goal_y, goal_x)\r\n",
                "      discount: Discounting factor included in all Timesteps.\r\n",
                "      penalty_for_walls: Reward added when hitting a wall (should be negative).\r\n",
                "      reward_goal: Reward added when finding the goal (should be positive).\r\n",
                "      max_episode_length: If set, will terminate an episode after this many\r\n",
                "        steps.\r\n",
                "      randomize_goals: If true, randomize goal at every episode.\r\n",
                "    \"\"\"\r\n",
                "    if observation_type not in ObservationType:\r\n",
                "      raise ValueError('observation_type should be a ObservationType instace.')\r\n",
                "    self._layout = np.array(layout)\r\n",
                "    self._start_state = start_state\r\n",
                "    self._state = self._start_state\r\n",
                "    self._number_of_states = np.prod(np.shape(self._layout))\r\n",
                "    self._discount = discount\r\n",
                "    self._penalty_for_walls = penalty_for_walls\r\n",
                "    self._reward_goal = reward_goal\r\n",
                "    self._observation_type = observation_type\r\n",
                "    self._layout_dims = self._layout.shape\r\n",
                "    self._max_episode_length = max_episode_length\r\n",
                "    self._num_episode_steps = 0\r\n",
                "    self._randomize_goals = randomize_goals\r\n",
                "    if goal_state is None:\r\n",
                "      # Randomly sample goal_state if not provided\r\n",
                "      goal_state = self._sample_goal()\r\n",
                "    self.goal_state = goal_state\r\n",
                "\r\n",
                "  def _sample_goal(self):\r\n",
                "    \"\"\"Randomly sample reachable non-starting state.\"\"\"\r\n",
                "    # Sample a new goal\r\n",
                "    n = 0\r\n",
                "    max_tries = 1e5\r\n",
                "    while n < max_tries:\r\n",
                "      goal_state = tuple(np.random.randint(d) for d in self._layout_dims)\r\n",
                "      if goal_state != self._state and self._layout[goal_state] == 0:\r\n",
                "        # Reachable state found!\r\n",
                "        return goal_state\r\n",
                "      n += 1\r\n",
                "    raise ValueError('Failed to sample a goal state.')\r\n",
                "\r\n",
                "  @property\r\n",
                "  def layout(self):\r\n",
                "    return self._layout\r\n",
                "\r\n",
                "  @property\r\n",
                "  def number_of_states(self):\r\n",
                "    return self._number_of_states\r\n",
                "\r\n",
                "  @property\r\n",
                "  def goal_state(self):\r\n",
                "    return self._goal_state\r\n",
                "\r\n",
                "  @property\r\n",
                "  def start_state(self):\r\n",
                "    return self._start_state\r\n",
                "\r\n",
                "  @property\r\n",
                "  def state(self):\r\n",
                "    return self._state\r\n",
                "\r\n",
                "  def set_state(self, x, y):\r\n",
                "    self._state = (y, x)\r\n",
                "\r\n",
                "  @goal_state.setter\r\n",
                "  def goal_state(self, new_goal):\r\n",
                "    if new_goal == self._state or self._layout[new_goal] < 0:\r\n",
                "      raise ValueError('This is not a valid goal!')\r\n",
                "    # Zero out any other goal\r\n",
                "    self._layout[self._layout > 0] = 0\r\n",
                "    # Setup new goal location\r\n",
                "    self._layout[new_goal] = self._reward_goal\r\n",
                "    self._goal_state = new_goal\r\n",
                "\r\n",
                "  def observation_spec(self):\r\n",
                "    if self._observation_type is ObservationType.AGENT_ONEHOT:\r\n",
                "      return specs.Array(\r\n",
                "          shape=self._layout_dims,\r\n",
                "          dtype=np.float32,\r\n",
                "          name='observation_agent_onehot')\r\n",
                "    elif self._observation_type is ObservationType.GRID:\r\n",
                "      return specs.Array(\r\n",
                "          shape=self._layout_dims + (3,),\r\n",
                "          dtype=np.float32,\r\n",
                "          name='observation_grid')\r\n",
                "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\r\n",
                "      return specs.Array(\r\n",
                "          shape=(4,), dtype=np.float32, name='observation_agent_goal_pos')\r\n",
                "    elif self._observation_type is ObservationType.STATE_INDEX:\r\n",
                "      return specs.DiscreteArray(\r\n",
                "          self._number_of_states, dtype=int, name='observation_state_index')\r\n",
                "\r\n",
                "  def action_spec(self):\r\n",
                "    return specs.DiscreteArray(4, dtype=int, name='action')\r\n",
                "\r\n",
                "  def get_obs(self):\r\n",
                "    if self._observation_type is ObservationType.AGENT_ONEHOT:\r\n",
                "      obs = np.zeros(self._layout.shape, dtype=np.float32)\r\n",
                "      # Place agent\r\n",
                "      obs[self._state] = 1\r\n",
                "      return obs\r\n",
                "    elif self._observation_type is ObservationType.GRID:\r\n",
                "      obs = np.zeros(self._layout.shape + (3,), dtype=np.float32)\r\n",
                "      obs[..., 0] = self._layout < 0\r\n",
                "      obs[self._state[0], self._state[1], 1] = 1\r\n",
                "      obs[self._goal_state[0], self._goal_state[1], 2] = 1\r\n",
                "      return obs\r\n",
                "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\r\n",
                "      return np.array(self._state + self._goal_state, dtype=np.float32)\r\n",
                "    elif self._observation_type is ObservationType.STATE_INDEX:\r\n",
                "      y, x = self._state\r\n",
                "      return y * self._layout.shape[1] + x\r\n",
                "\r\n",
                "  def reset(self):\r\n",
                "    self._state = self._start_state\r\n",
                "    self._num_episode_steps = 0\r\n",
                "    if self._randomize_goals:\r\n",
                "      self.goal_state = self._sample_goal()\r\n",
                "    return dm_env.TimeStep(\r\n",
                "        step_type=dm_env.StepType.FIRST,\r\n",
                "        reward=None,\r\n",
                "        discount=None,\r\n",
                "        observation=self.get_obs())\r\n",
                "\r\n",
                "  def step(self, action):\r\n",
                "    y, x = self._state\r\n",
                "\r\n",
                "    if action == 0:  # up\r\n",
                "      new_state = (y - 1, x)\r\n",
                "    elif action == 1:  # right\r\n",
                "      new_state = (y, x + 1)\r\n",
                "    elif action == 2:  # down\r\n",
                "      new_state = (y + 1, x)\r\n",
                "    elif action == 3:  # left\r\n",
                "      new_state = (y, x - 1)\r\n",
                "    else:\r\n",
                "      raise ValueError(\r\n",
                "          'Invalid action: {} is not 0, 1, 2, or 3.'.format(action))\r\n",
                "\r\n",
                "    new_y, new_x = new_state\r\n",
                "    step_type = dm_env.StepType.MID\r\n",
                "    if self._layout[new_y, new_x] == -1:  # wall\r\n",
                "      reward = self._penalty_for_walls\r\n",
                "      discount = self._discount\r\n",
                "      new_state = (y, x)\r\n",
                "    elif self._layout[new_y, new_x] == 0:  # empty cell\r\n",
                "      reward = 0.\r\n",
                "      discount = self._discount\r\n",
                "    else:  # a goal\r\n",
                "      reward = self._layout[new_y, new_x]\r\n",
                "      discount = 0.\r\n",
                "      new_state = self._start_state\r\n",
                "      step_type = dm_env.StepType.LAST\r\n",
                "\r\n",
                "    self._state = new_state\r\n",
                "    self._num_episode_steps += 1\r\n",
                "    if (self._max_episode_length is not None and\r\n",
                "            self._num_episode_steps >= self._max_episode_length):\r\n",
                "      step_type = dm_env.StepType.LAST\r\n",
                "    return dm_env.TimeStep(\r\n",
                "        step_type=step_type,\r\n",
                "        reward=np.float32(reward),\r\n",
                "        discount=discount,\r\n",
                "        observation=self.get_obs())\r\n",
                "\r\n",
                "  def plot_grid(self, add_start=True):\r\n",
                "    plt.figure(figsize=(4, 4))\r\n",
                "    plt.imshow(self._layout <= -1, interpolation='nearest')\r\n",
                "    ax = plt.gca()\r\n",
                "    ax.grid(0)\r\n",
                "    plt.xticks([])\r\n",
                "    plt.yticks([])\r\n",
                "    # Add start/goal\r\n",
                "    if add_start:\r\n",
                "      plt.text(\r\n",
                "          self._start_state[1],\r\n",
                "          self._start_state[0],\r\n",
                "          r'$\\mathbf{S}$',\r\n",
                "          fontsize=16,\r\n",
                "          ha='center',\r\n",
                "          va='center')\r\n",
                "    plt.text(\r\n",
                "        self._goal_state[1],\r\n",
                "        self._goal_state[0],\r\n",
                "        r'$\\mathbf{G}$',\r\n",
                "        fontsize=16,\r\n",
                "        ha='center',\r\n",
                "        va='center')\r\n",
                "    h, w = self._layout.shape\r\n",
                "    for y in range(h - 1):\r\n",
                "      plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-w', lw=2)\r\n",
                "    for x in range(w - 1):\r\n",
                "      plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-w', lw=2)\r\n",
                "\r\n",
                "  def plot_state(self, return_rgb=False):\r\n",
                "    self.plot_grid(add_start=False)\r\n",
                "    # Add the agent location\r\n",
                "    plt.text(\r\n",
                "        self._state[1],\r\n",
                "        self._state[0],\r\n",
                "        u'😃',\r\n",
                "        # fontname='symbola',\r\n",
                "        fontsize=18,\r\n",
                "        ha='center',\r\n",
                "        va='center',\r\n",
                "    )\r\n",
                "    if return_rgb:\r\n",
                "      fig = plt.gcf()\r\n",
                "      plt.axis('tight')\r\n",
                "      plt.subplots_adjust(0, 0, 1, 1, 0, 0)\r\n",
                "      fig.canvas.draw()\r\n",
                "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\r\n",
                "      w, h = fig.canvas.get_width_height()\r\n",
                "      data = data.reshape((h, w, 3))\r\n",
                "      plt.close(fig)\r\n",
                "      return data\r\n",
                "\r\n",
                "  def plot_policy(self, policy):\r\n",
                "    action_names = [\r\n",
                "        r'$\\uparrow$', r'$\\rightarrow$', r'$\\downarrow$', r'$\\leftarrow$'\r\n",
                "    ]\r\n",
                "    self.plot_grid()\r\n",
                "    plt.title('Policy Visualization')\r\n",
                "    h, w = self._layout.shape\r\n",
                "    for y in range(h):\r\n",
                "      for x in range(w):\r\n",
                "        # if ((y, x) != self._start_state) and ((y, x) != self._goal_state):\r\n",
                "        if (y, x) != self._goal_state:\r\n",
                "          action_name = action_names[policy[y, x]]\r\n",
                "          plt.text(x, y, action_name, ha='center', va='center')\r\n",
                "\r\n",
                "  def plot_greedy_policy(self, q):\r\n",
                "    greedy_actions = np.argmax(q, axis=2)\r\n",
                "    self.plot_policy(greedy_actions)\r\n",
                "\r\n",
                "\r\n",
                "def build_gridworld_task(task,\r\n",
                "                         discount=0.9,\r\n",
                "                         penalty_for_walls=-5,\r\n",
                "                         observation_type=ObservationType.STATE_INDEX,\r\n",
                "                         max_episode_length=200):\r\n",
                "  \"\"\"Construct a particular Gridworld layout with start/goal states.\r\n",
                "\r\n",
                "  Args:\r\n",
                "      task: string name of the task to use. One of {'simple', 'obstacle',\r\n",
                "        'random_goal'}.\r\n",
                "      discount: Discounting factor included in all Timesteps.\r\n",
                "      penalty_for_walls: Reward added when hitting a wall (should be negative).\r\n",
                "      observation_type: Enum observation type to use. One of:\r\n",
                "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\r\n",
                "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\r\n",
                "          agent is and 0 elsewhere.\r\n",
                "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\r\n",
                "          First channel contains walls (1 if wall, 0 otherwise), second the\r\n",
                "          agent position (1 if agent, 0 otherwise) and third goal position\r\n",
                "          (1 if goal, 0 otherwise)\r\n",
                "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\r\n",
                "          (agent_y, agent_x, goal_y, goal_x).\r\n",
                "      max_episode_length: If set, will terminate an episode after this many\r\n",
                "        steps.\r\n",
                "  \"\"\"\r\n",
                "  tasks_specifications = {\r\n",
                "      'simple': {\r\n",
                "          'layout': layout,\r\n",
                "\r\n",
                "          'start_state': (2, 2),\r\n",
                "          'goal_state': (7, 2)\r\n",
                "      },\r\n",
                "     \r\n",
                "  }\r\n",
                "  return GridWorld(\r\n",
                "      discount=discount,\r\n",
                "      penalty_for_walls=penalty_for_walls,\r\n",
                "      observation_type=observation_type,\r\n",
                "      max_episode_length=max_episode_length,\r\n",
                "      **tasks_specifications[task])\r\n",
                "\r\n",
                "\r\n",
                "def setup_environment(environment):\r\n",
                "  \"\"\"Returns the environment and its spec.\"\"\"\r\n",
                "\r\n",
                "  # Make sure the environment outputs single-precision floats.\r\n",
                "  environment = wrappers.SinglePrecisionWrapper(environment)\r\n",
                "\r\n",
                "  # Grab the spec of the environment.\r\n",
                "  environment_spec = specs.make_environment_spec(environment)\r\n",
                "\r\n",
                "  return environment, environment_spec\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Hand-coding\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 144,
            "source": [
                "chunk = [\r\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1],\r\n",
                "    [-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1],\r\n",
                "    [-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1],\r\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1, 0,\r\n",
                "     0, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
                "]\r\n",
                "\r\n",
                "\r\n",
                "width = 2\r\n",
                "height = 4\r\n",
                "\r\n",
                "layout = np.tile(chunk, (1, 2))\r\n",
                "layout = np.concatenate((layout, np.flip(layout, axis=0)), axis=0)\r\n",
                "layout = np.concatenate((layout, layout), axis=0)\r\n",
                "\r\n",
                "path = np.zeros(layout.shape[1])\r\n",
                "path[int(len(path)/2):] = -1\r\n",
                "path = np.tile(path, (2, 1))\r\n",
                "\r\n",
                "layout = np.insert(layout, int(layout.shape[0]/2), path, axis=0)\r\n",
                "\r\n",
                "path = np.zeros(layout.shape[0])\r\n",
                "path[:int(len(path)/4)] = -1\r\n",
                "path[int(len(path)*3/4) +1:] = -1\r\n",
                "path = np.tile(path, (2, 1))\r\n",
                "\r\n",
                "layout = np.insert(layout, int(layout.shape[1]/2), path, axis=1)\r\n",
                "\r\n",
                "path = np.zeros(layout.shape[1])\r\n",
                "path[:int(len(path)/4 -1)] = -1\r\n",
                "path[int(len(path)*3/4)+2:] = -1\r\n",
                "path = np.tile(path, (2, 1))\r\n",
                "\r\n",
                "layout = np.insert(layout, int(layout.shape[1]/4)+1, path, axis=0)\r\n",
                "layout = np.insert(layout, int(layout.shape[1]*4/5)+4, path, axis=0)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Recursive"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "source": [
                "layout = np.array([\r\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1],\r\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1]\r\n",
                "    \r\n",
                "])\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Visualize"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 182,
            "source": [
                "plt.figure(figsize=(4, 4))\r\n",
                "plt.imshow(layout)\r\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "<matplotlib.image.AxesImage at 0x24fa8a59850>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 182
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 288x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "image/png": {
                            "width": 270,
                            "height": 154
                        },
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 146,
            "source": [
                "# Visualise GridWorlds\r\n",
                "\r\n",
                "# Instantiate two tabular environments, a simple task, and one that involves\r\n",
                "# the avoidance of an obstacle.\r\n",
                "simple_grid = build_gridworld_task(\r\n",
                "    task='simple', observation_type=ObservationType.GRID)\r\n",
                "\r\n",
                "# Plot them.\r\n",
                "simple_grid.plot_grid()\r\n",
                "plt.title('Simple')\r\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "Text(0.5, 1.0, 'Simple')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 146
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 288x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "image/png": {
                            "width": 210,
                            "height": 269
                        }
                    }
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('nmarl': conda)"
        },
        "interpreter": {
            "hash": "16466b8fba2a6ac6e61862ec7dae953cfc93f6c9e5771a8e7fd97f2453f609b9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}