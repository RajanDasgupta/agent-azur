{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "%run MM_Maze_Utils.py\n",
                "%run MM_Plot_Utils.py"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "UsageError: Line magic function `%` not found.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Set up"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 149,
            "source": [
                "# Import modules\n",
                "import gym\n",
                "import enum\n",
                "import copy\n",
                "import time\n",
                "import acme\n",
                "import torch\n",
                "import base64\n",
                "import dm_env\n",
                "import IPython\n",
                "import imageio\n",
                "import warnings\n",
                "import itertools\n",
                "import collections\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "import matplotlib as mpl\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import tensorflow.compat.v2 as tf\n",
                "\n",
                "from acme import specs\n",
                "from acme import wrappers\n",
                "from acme.utils import tree_utils\n",
                "from acme.utils import loggers\n",
                "from torch.autograd import Variable\n",
                "from torch.distributions import Categorical\n",
                "from typing import Callable, Sequence\n",
                "\n",
                "tf.enable_v2_behavior()\n",
                "warnings.filterwarnings('ignore')\n",
                "np.set_printoptions(precision=3, suppress=1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# brief import\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib as mpl\n",
                "import matplotlib.pyplot as plt"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# @title Figure settings\n",
                "import ipywidgets as widgets       # interactive display\n",
                "%matplotlib inline\n",
                "%config InlineBackend.figure_format = 'retina'\n",
                "plt.style.use(\n",
                "    \"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
                "mpl.rc('image', cmap='Blues')\n",
                "\n",
                "def map_from_action_to_subplot(a): return (2, 6, 8, 4)[a]\n",
                "def map_from_action_to_name(a): return (\"up\", \"right\", \"down\", \"left\")[a]\n",
                "\n",
                "\n",
                "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
                "  plt.imshow(values, interpolation=\"nearest\",\n",
                "             cmap=colormap, vmin=vmin, vmax=vmax)\n",
                "  plt.yticks([])\n",
                "  plt.xticks([])\n",
                "  plt.colorbar(ticks=[vmin, vmax])\n",
                "\n",
                "\n",
                "def plot_state_value(action_values, epsilon=0.1):\n",
                "  q = action_values\n",
                "  fig = plt.figure(figsize=(4, 4))\n",
                "  vmin = np.min(action_values)\n",
                "  vmax = np.max(action_values)\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
                "  plt.title(\"$v(s)$\")\n",
                "\n",
                "\n",
                "def plot_action_values(action_values, epsilon=0.1):\n",
                "  q = action_values\n",
                "  fig = plt.figure(figsize=(8, 8))\n",
                "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "  vmin = np.min(action_values)\n",
                "  vmax = np.max(action_values)\n",
                "  dif = vmax - vmin\n",
                "  for a in [0, 1, 2, 3]:\n",
                "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
                "\n",
                "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
                "    action_name = map_from_action_to_name(a)\n",
                "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
                "\n",
                "  plt.subplot(3, 3, 5)\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
                "  plt.title(\"$v(s)$\")\n",
                "\n",
                "\n",
                "def plot_stats(stats, window=10):\n",
                "  plt.figure(figsize=(16, 4))\n",
                "  plt.subplot(121)\n",
                "  xline = range(0, len(stats.episode_lengths), window)\n",
                "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
                "  plt.ylabel('Episode Length')\n",
                "  plt.xlabel('Episode Count')\n",
                "  plt.subplot(122)\n",
                "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
                "  plt.ylabel('Episode Return')\n",
                "  plt.xlabel('Episode Count')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def map_from_action_to_subplot(a): return (2, 6, 8, 4)[a]\r\n",
                "def map_from_action_to_name(a): return (\"up\", \"right\", \"down\", \"left\")[a]\r\n",
                "\r\n",
                "\r\n",
                "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\r\n",
                "  plt.imshow(values, interpolation=\"nearest\",\r\n",
                "             cmap=colormap, vmin=vmin, vmax=vmax)\r\n",
                "  plt.yticks([])\r\n",
                "  plt.xticks([])\r\n",
                "  plt.colorbar(ticks=[vmin, vmax])\r\n",
                "\r\n",
                "\r\n",
                "def plot_state_value(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(4, 4))\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_action_values(action_values, epsilon=0.1):\r\n",
                "  q = action_values\r\n",
                "  fig = plt.figure(figsize=(8, 8))\r\n",
                "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\r\n",
                "  vmin = np.min(action_values)\r\n",
                "  vmax = np.max(action_values)\r\n",
                "  dif = vmax - vmin\r\n",
                "  for a in [0, 1, 2, 3]:\r\n",
                "    plt.subplot(3, 3, map_from_action_to_subplot(a))\r\n",
                "\r\n",
                "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\r\n",
                "    action_name = map_from_action_to_name(a)\r\n",
                "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\r\n",
                "\r\n",
                "  plt.subplot(3, 3, 5)\r\n",
                "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
                "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
                "  plt.title(\"$v(s)$\")\r\n",
                "\r\n",
                "\r\n",
                "def plot_stats(stats, window=10):\r\n",
                "  plt.figure(figsize=(16, 4))\r\n",
                "  plt.subplot(121)\r\n",
                "  xline = range(0, len(stats.episode_lengths), window)\r\n",
                "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\r\n",
                "  plt.ylabel('Episode Length')\r\n",
                "  plt.xlabel('Episode Count')\r\n",
                "  plt.subplot(122)\r\n",
                "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\r\n",
                "  plt.ylabel('Episode Return')\r\n",
                "  plt.xlabel('Episode Count')\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Grid-world"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 145,
            "source": [
                "# @title Implement GridWorld { form-width: \"30%\" }\n",
                "# @markdown *Double-click* to inspect the contents of this cell.\n",
                "\n",
                "class ObservationType(enum.IntEnum):\n",
                "  STATE_INDEX = enum.auto()\n",
                "  AGENT_ONEHOT = enum.auto()\n",
                "  GRID = enum.auto()\n",
                "  AGENT_GOAL_POS = enum.auto()\n",
                "\n",
                "\n",
                "class GridWorld(dm_env.Environment):\n",
                "\n",
                "  def __init__(self,\n",
                "               layout,\n",
                "               start_state,\n",
                "               goal_state=None,\n",
                "               observation_type=ObservationType.STATE_INDEX,\n",
                "               discount=0.9,\n",
                "               penalty_for_walls=-5,\n",
                "               reward_goal=10,\n",
                "               max_episode_length=None,\n",
                "               randomize_goals=False):\n",
                "    \"\"\"Build a grid environment.\n",
                "\n",
                "    Simple gridworld defined by a map layout, a start and a goal state.\n",
                "\n",
                "    Layout should be a NxN grid, containing:\n",
                "      * 0: empty\n",
                "      * -1: wall\n",
                "      * Any other positive value: value indicates reward; episode will terminate\n",
                "\n",
                "    Args:\n",
                "      layout: NxN array of numbers, indicating the layout of the environment.\n",
                "      start_state: Tuple (y, x) of starting location.\n",
                "      goal_state: Optional tuple (y, x) of goal location. Will be randomly\n",
                "        sampled once if None.\n",
                "      observation_type: Enum observation type to use. One of:\n",
                "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
                "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\n",
                "          agent is and 0 elsewhere.\n",
                "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\n",
                "          First channel contains walls (1 if wall, 0 otherwise), second the\n",
                "          agent position (1 if agent, 0 otherwise) and third goal position\n",
                "          (1 if goal, 0 otherwise)\n",
                "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\n",
                "          (agent_y, agent_x, goal_y, goal_x)\n",
                "      discount: Discounting factor included in all Timesteps.\n",
                "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
                "      reward_goal: Reward added when finding the goal (should be positive).\n",
                "      max_episode_length: If set, will terminate an episode after this many\n",
                "        steps.\n",
                "      randomize_goals: If true, randomize goal at every episode.\n",
                "    \"\"\"\n",
                "    if observation_type not in ObservationType:\n",
                "      raise ValueError('observation_type should be a ObservationType instace.')\n",
                "    self._layout = np.array(layout)\n",
                "    self._start_state = start_state\n",
                "    self._state = self._start_state\n",
                "    self._number_of_states = np.prod(np.shape(self._layout))\n",
                "    self._discount = discount\n",
                "    self._penalty_for_walls = penalty_for_walls\n",
                "    self._reward_goal = reward_goal\n",
                "    self._observation_type = observation_type\n",
                "    self._layout_dims = self._layout.shape\n",
                "    self._max_episode_length = max_episode_length\n",
                "    self._num_episode_steps = 0\n",
                "    self._randomize_goals = randomize_goals\n",
                "    if goal_state is None:\n",
                "      # Randomly sample goal_state if not provided\n",
                "      goal_state = self._sample_goal()\n",
                "    self.goal_state = goal_state\n",
                "\n",
                "  def _sample_goal(self):\n",
                "    \"\"\"Randomly sample reachable non-starting state.\"\"\"\n",
                "    # Sample a new goal\n",
                "    n = 0\n",
                "    max_tries = 1e5\n",
                "    while n < max_tries:\n",
                "      goal_state = tuple(np.random.randint(d) for d in self._layout_dims)\n",
                "      if goal_state != self._state and self._layout[goal_state] == 0:\n",
                "        # Reachable state found!\n",
                "        return goal_state\n",
                "      n += 1\n",
                "    raise ValueError('Failed to sample a goal state.')\n",
                "\n",
                "  @property\n",
                "  def layout(self):\n",
                "    return self._layout\n",
                "\n",
                "  @property\n",
                "  def number_of_states(self):\n",
                "    return self._number_of_states\n",
                "\n",
                "  @property\n",
                "  def goal_state(self):\n",
                "    return self._goal_state\n",
                "\n",
                "  @property\n",
                "  def start_state(self):\n",
                "    return self._start_state\n",
                "\n",
                "  @property\n",
                "  def state(self):\n",
                "    return self._state\n",
                "\n",
                "  def set_state(self, x, y):\n",
                "    self._state = (y, x)\n",
                "\n",
                "  @goal_state.setter\n",
                "  def goal_state(self, new_goal):\n",
                "    if new_goal == self._state or self._layout[new_goal] < 0:\n",
                "      raise ValueError('This is not a valid goal!')\n",
                "    # Zero out any other goal\n",
                "    self._layout[self._layout > 0] = 0\n",
                "    # Setup new goal location\n",
                "    self._layout[new_goal] = self._reward_goal\n",
                "    self._goal_state = new_goal\n",
                "\n",
                "  def observation_spec(self):\n",
                "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
                "      return specs.Array(\n",
                "          shape=self._layout_dims,\n",
                "          dtype=np.float32,\n",
                "          name='observation_agent_onehot')\n",
                "    elif self._observation_type is ObservationType.GRID:\n",
                "      return specs.Array(\n",
                "          shape=self._layout_dims + (3,),\n",
                "          dtype=np.float32,\n",
                "          name='observation_grid')\n",
                "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
                "      return specs.Array(\n",
                "          shape=(4,), dtype=np.float32, name='observation_agent_goal_pos')\n",
                "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
                "      return specs.DiscreteArray(\n",
                "          self._number_of_states, dtype=int, name='observation_state_index')\n",
                "\n",
                "  def action_spec(self):\n",
                "    return specs.DiscreteArray(4, dtype=int, name='action')\n",
                "\n",
                "  def get_obs(self):\n",
                "    if self._observation_type is ObservationType.AGENT_ONEHOT:\n",
                "      obs = np.zeros(self._layout.shape, dtype=np.float32)\n",
                "      # Place agent\n",
                "      obs[self._state] = 1\n",
                "      return obs\n",
                "    elif self._observation_type is ObservationType.GRID:\n",
                "      obs = np.zeros(self._layout.shape + (3,), dtype=np.float32)\n",
                "      obs[..., 0] = self._layout < 0\n",
                "      obs[self._state[0], self._state[1], 1] = 1\n",
                "      obs[self._goal_state[0], self._goal_state[1], 2] = 1\n",
                "      return obs\n",
                "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\n",
                "      return np.array(self._state + self._goal_state, dtype=np.float32)\n",
                "    elif self._observation_type is ObservationType.STATE_INDEX:\n",
                "      y, x = self._state\n",
                "      return y * self._layout.shape[1] + x\n",
                "\n",
                "  def reset(self):\n",
                "    self._state = self._start_state\n",
                "    self._num_episode_steps = 0\n",
                "    if self._randomize_goals:\n",
                "      self.goal_state = self._sample_goal()\n",
                "    return dm_env.TimeStep(\n",
                "        step_type=dm_env.StepType.FIRST,\n",
                "        reward=None,\n",
                "        discount=None,\n",
                "        observation=self.get_obs())\n",
                "\n",
                "  def step(self, action):\n",
                "    y, x = self._state\n",
                "\n",
                "    if action == 0:  # up\n",
                "      new_state = (y - 1, x)\n",
                "    elif action == 1:  # right\n",
                "      new_state = (y, x + 1)\n",
                "    elif action == 2:  # down\n",
                "      new_state = (y + 1, x)\n",
                "    elif action == 3:  # left\n",
                "      new_state = (y, x - 1)\n",
                "    else:\n",
                "      raise ValueError(\n",
                "          'Invalid action: {} is not 0, 1, 2, or 3.'.format(action))\n",
                "\n",
                "    new_y, new_x = new_state\n",
                "    step_type = dm_env.StepType.MID\n",
                "    if self._layout[new_y, new_x] == -1:  # wall\n",
                "      reward = self._penalty_for_walls\n",
                "      discount = self._discount\n",
                "      new_state = (y, x)\n",
                "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
                "      reward = 0.\n",
                "      discount = self._discount\n",
                "    else:  # a goal\n",
                "      reward = self._layout[new_y, new_x]\n",
                "      discount = 0.\n",
                "      new_state = self._start_state\n",
                "      step_type = dm_env.StepType.LAST\n",
                "\n",
                "    self._state = new_state\n",
                "    self._num_episode_steps += 1\n",
                "    if (self._max_episode_length is not None and\n",
                "            self._num_episode_steps >= self._max_episode_length):\n",
                "      step_type = dm_env.StepType.LAST\n",
                "    return dm_env.TimeStep(\n",
                "        step_type=step_type,\n",
                "        reward=np.float32(reward),\n",
                "        discount=discount,\n",
                "        observation=self.get_obs())\n",
                "\n",
                "  def plot_grid(self, add_start=True):\n",
                "    plt.figure(figsize=(4, 4))\n",
                "    plt.imshow(self._layout <= -1, interpolation='nearest')\n",
                "    ax = plt.gca()\n",
                "    ax.grid(0)\n",
                "    plt.xticks([])\n",
                "    plt.yticks([])\n",
                "    # Add start/goal\n",
                "    if add_start:\n",
                "      plt.text(\n",
                "          self._start_state[1],\n",
                "          self._start_state[0],\n",
                "          r'$\\mathbf{S}$',\n",
                "          fontsize=16,\n",
                "          ha='center',\n",
                "          va='center')\n",
                "    plt.text(\n",
                "        self._goal_state[1],\n",
                "        self._goal_state[0],\n",
                "        r'$\\mathbf{G}$',\n",
                "        fontsize=16,\n",
                "        ha='center',\n",
                "        va='center')\n",
                "    h, w = self._layout.shape\n",
                "    for y in range(h - 1):\n",
                "      plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-w', lw=2)\n",
                "    for x in range(w - 1):\n",
                "      plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-w', lw=2)\n",
                "\n",
                "  def plot_state(self, return_rgb=False):\n",
                "    self.plot_grid(add_start=False)\n",
                "    # Add the agent location\n",
                "    plt.text(\n",
                "        self._state[1],\n",
                "        self._state[0],\n",
                "        u'😃',\n",
                "        # fontname='symbola',\n",
                "        fontsize=18,\n",
                "        ha='center',\n",
                "        va='center',\n",
                "    )\n",
                "    if return_rgb:\n",
                "      fig = plt.gcf()\n",
                "      plt.axis('tight')\n",
                "      plt.subplots_adjust(0, 0, 1, 1, 0, 0)\n",
                "      fig.canvas.draw()\n",
                "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
                "      w, h = fig.canvas.get_width_height()\n",
                "      data = data.reshape((h, w, 3))\n",
                "      plt.close(fig)\n",
                "      return data\n",
                "\n",
                "  def plot_policy(self, policy):\n",
                "    action_names = [\n",
                "        r'$\\uparrow$', r'$\\rightarrow$', r'$\\downarrow$', r'$\\leftarrow$'\n",
                "    ]\n",
                "    self.plot_grid()\n",
                "    plt.title('Policy Visualization')\n",
                "    h, w = self._layout.shape\n",
                "    for y in range(h):\n",
                "      for x in range(w):\n",
                "        # if ((y, x) != self._start_state) and ((y, x) != self._goal_state):\n",
                "        if (y, x) != self._goal_state:\n",
                "          action_name = action_names[policy[y, x]]\n",
                "          plt.text(x, y, action_name, ha='center', va='center')\n",
                "\n",
                "  def plot_greedy_policy(self, q):\n",
                "    greedy_actions = np.argmax(q, axis=2)\n",
                "    self.plot_policy(greedy_actions)\n",
                "\n",
                "\n",
                "def build_gridworld_task(task,\n",
                "                         discount=0.9,\n",
                "                         penalty_for_walls=-5,\n",
                "                         observation_type=ObservationType.STATE_INDEX,\n",
                "                         max_episode_length=200):\n",
                "  \"\"\"Construct a particular Gridworld layout with start/goal states.\n",
                "\n",
                "  Args:\n",
                "      task: string name of the task to use. One of {'simple', 'obstacle',\n",
                "        'random_goal'}.\n",
                "      discount: Discounting factor included in all Timesteps.\n",
                "      penalty_for_walls: Reward added when hitting a wall (should be negative).\n",
                "      observation_type: Enum observation type to use. One of:\n",
                "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\n",
                "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\n",
                "          agent is and 0 elsewhere.\n",
                "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\n",
                "          First channel contains walls (1 if wall, 0 otherwise), second the\n",
                "          agent position (1 if agent, 0 otherwise) and third goal position\n",
                "          (1 if goal, 0 otherwise)\n",
                "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\n",
                "          (agent_y, agent_x, goal_y, goal_x).\n",
                "      max_episode_length: If set, will terminate an episode after this many\n",
                "        steps.\n",
                "  \"\"\"\n",
                "  tasks_specifications = {\n",
                "      'simple': {\n",
                "          'layout': layout,\n",
                "\n",
                "          'start_state': (2, 2),\n",
                "          'goal_state': (7, 2)\n",
                "      },\n",
                "     \n",
                "  }\n",
                "  return GridWorld(\n",
                "      discount=discount,\n",
                "      penalty_for_walls=penalty_for_walls,\n",
                "      observation_type=observation_type,\n",
                "      max_episode_length=max_episode_length,\n",
                "      **tasks_specifications[task])\n",
                "\n",
                "\n",
                "def setup_environment(environment):\n",
                "  \"\"\"Returns the environment and its spec.\"\"\"\n",
                "\n",
                "  # Make sure the environment outputs single-precision floats.\n",
                "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
                "\n",
                "  # Grab the spec of the environment.\n",
                "  environment_spec = specs.make_environment_spec(environment)\n",
                "\n",
                "  return environment, environment_spec\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Hand-coding\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 144,
            "source": [
                "chunk = [\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1],\n",
                "    [-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1],\n",
                "    [-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1],\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1, 0,\n",
                "     0, -1, -1, -1, -1, -1, -1, -1, -1],\n",
                "]\n",
                "\n",
                "\n",
                "width = 2\n",
                "height = 4\n",
                "\n",
                "layout = np.tile(chunk, (1, 2))\n",
                "layout = np.concatenate((layout, np.flip(layout, axis=0)), axis=0)\n",
                "layout = np.concatenate((layout, layout), axis=0)\n",
                "\n",
                "path = np.zeros(layout.shape[1])\n",
                "path[int(len(path)/2):] = -1\n",
                "path = np.tile(path, (2, 1))\n",
                "\n",
                "layout = np.insert(layout, int(layout.shape[0]/2), path, axis=0)\n",
                "\n",
                "path = np.zeros(layout.shape[0])\n",
                "path[:int(len(path)/4)] = -1\n",
                "path[int(len(path)*3/4) +1:] = -1\n",
                "path = np.tile(path, (2, 1))\n",
                "\n",
                "layout = np.insert(layout, int(layout.shape[1]/2), path, axis=1)\n",
                "\n",
                "path = np.zeros(layout.shape[1])\n",
                "path[:int(len(path)/4 -1)] = -1\n",
                "path[int(len(path)*3/4)+2:] = -1\n",
                "path = np.tile(path, (2, 1))\n",
                "\n",
                "layout = np.insert(layout, int(layout.shape[1]/4)+1, path, axis=0)\n",
                "layout = np.insert(layout, int(layout.shape[1]*4/5)+4, path, axis=0)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Fractal"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 122,
            "source": [
                "layout = np.array([\n",
                "    [-1, -1, -1, -1, -1, -1, -1, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, 0, 0, 0, 0, 0, 0, -1],\n",
                "    [-1, -1, -1, 0, 0, -1, -1, -1]\n",
                "])\n",
                "\n",
                "n = 5 # number of times to repeat\n",
                "\n",
                "def fractal_maze (layout, n=5):\n",
                "    \"\"\"\n",
                "    Returns binary maze with layout matrix repeated 2^n times\n",
                "    \"\"\"\n",
                "    for i in range(n):\n",
                "        div = i%2 == 0\n",
                "\n",
                "        # Flip\n",
                "        flipped = np.flip(layout, axis=int(not div))\n",
                "    \n",
                "        # Construct entry path\n",
                "        path = np.zeros(layout.shape[int(div)])\n",
                "        path[:int(len(path)/2) - 1] = -1\n",
                "\n",
                "        # Concatenate\n",
                "        if div:\n",
                "            path = np.tile(path, (2, 1))\n",
                "            layout = np.vstack((layout, path, flipped))\n",
                "        else:\n",
                "            path = np.tile(path.reshape(len(path), 1), (1, 2))\n",
                "            layout = np.hstack((layout, path, flipped))\n",
                "\n",
                "    return layout\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Visualize"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 133,
            "source": [
                "plt.figure(figsize=(20, 20))\n",
                "plt.imshow(fractal_maze(layout, 10))"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "<matplotlib.image.AxesImage at 0x7fa4e27af970>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 133
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 1440x1440 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 146,
            "source": [
                "# Visualise GridWorlds\n",
                "\n",
                "# Instantiate two tabular environments, a simple task, and one that involves\n",
                "# the avoidance of an obstacle.\n",
                "simple_grid = build_gridworld_task(\n",
                "    task='simple', observation_type=ObservationType.GRID)\n",
                "\n",
                "# Plot them.\n",
                "simple_grid.plot_grid()\n",
                "plt.title('Simple')\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "Text(0.5, 1.0, 'Simple')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 146
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 288x288 with 1 Axes>"
                        ],
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAIbCAYAAACkOOoIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAAAcs0lEQVR4nO3de5SkZX0n8O/DjDIDDJgBRNgQs7oGMVG5qyCXBRU0By9AuMSYYzSe7K4kZgmbZKOb25qL65pljZ49khPRLLksyCoaISIYCBAgCbgac0jIRQRUVO4jDjLMvPtH9UDNTM1M13RV9++t+XzO6cN01beq3u53er5U1/P8qnVdFwBYarss9QEAQKKQAChCIQFQgkICoASFBEAJCgmAEhQSACUoJABKUEgAlKCQAChBIQFQgkICoASFBEAJComZ0Vq7trXWzX386lIfz1Jrrd059P1481IfD2zP8qU+AHZurbW9kpyd5MQkhyTZN8meSb6b5OEkX0nyj0luS3Jzkr/uum79khwsMFUKiSXRWluW5OeS/EqS3UZEdpv72D/JS5O8ae7yh1prp3Rdd8uiHCiwaBQSi6619rQklyR5/WZXPZ7kjiT3JWlJ9k7yvCS7DmWekeR7pn6QwKJTSCyFX8+mZfSPSf5Lkk92Xbd2ODhXXocmeW2SH0nyA1u7067rTpj0gQKLp3kLcxZTa22/JHcnedrcRV9MclzXdQ/P47YtyUlJ7uq67o7pHeVsaK3dmeTZc5/+RNd1H1m6o4Ht8wyJxXZqniqjJPlP8ymjJOkG//d09VSOClhyln2z2J6/2ec3TuqO57vse9Ry6NbastbaGa21y1tr/9Jae6y19lBr7frW2tvmFmFsfj+rWms/11q7obX2YGvt8dba11prl7XWTprnMY86lqe11s5urf1pa+3Lc8fyjdbaX7TW3tFa232HvkHzO57WWvvh1tqHWmtfaq3dN/d13dtau7G19iutte+d1uOzc/MMicW252aftyU5iuEDaO2AJH+U5PjNrto1ycvnPs5qrZ268TWu1tpxSf4kg1WAw/ZPclqS01prv9N13c+NeSwHZrDg46WbXfXMuY9jk/xsa+2NXdf95Tj3PY/HPjLJ/0py+Iir95v7ODrJL7TW3t113W9O8vHBMyQW232bff6qJTmKp+yR5Ko8VUZfTnJtkr/KYNXfRicl+d9J0lo7NslnMiifLsmXknxu7r/DzmutvX3MY7k6T5XRvUmunzuWR4dy35/kM621l41x39vUWntdkuuyaRl9O8lf56mvbcPc5SuT/EZr7cJJPT4kSbqu8+Fj0T4yeA2pG/r4apIjJ3Tf1w7d769uI3fnUO6+uf/emOTQzXJ7J/nEZsd7cpKvzf3595Psv9ltfijJPwzlH06yxzyP5Ztz/70ryQ9nbtHRXG63JD+fwYbhjfmvJFk1z/t+8zZyL06ydih7R5I3JFm+We5ZSX5vs+/HTy713ykfs/PhGRKL7TMZ/EO50QFJbmmtXdNaO7e19uJRr9dM0d4ZPDM4seu6zw9f0XXd/UnOTPIvQxd/MoNnRu/puu6tXdd9fbPbfCmDJe0bp0nsmcGv8OZj3wxK6biu6z7ddd2TS2C7rvtO13X/LU9tEE6S70vyS/O875Faa7sk+cMkK+YuuiXJ4V3XfbzruieGs13X3dt13duSvHvo4t9ura1cyDHARgqJRdV13eNJfjyD/9PfqGUwOuh3k/y/JGtaaze31v7n3EKDvaZ4SOszWBL93VFXzh3v7w9d9PQMnkG8a2t32HXd7Un+fOiil49xPL/Qdd2d27jvS5JcNnTRT7bWdt1afh5em+QH5/68NsmZXdet2c5tfiWD70EyKPRzFvD48CSFxKLruu76JMck+butRFYmeUmSn0lyaZJ7W2sXt9a2uil2AT7bdd2Xt5O5ebPPP7z5s4ft3OYF8zyWBzJ4trI97x/68z5Jjpvn/Y/y5qE/X9p13V3bu0HXdRuSXDx00SsW8PjwJIXEkui67tYkL0pyRpJPJXlsG/EVSd6Y5O9aa++Y8KHcNI/MvTtwm+Ff5c131NE1Xdetm0fu+iTDz2JeMs/738TcRuNjhy4aZ4/XF4f+fMSOPD5szrJvlszc/2lfluSyuV87HZnkqAymfr8kW44JWp7kgtbahq7rfndCh7F52YzynQXeZtTw2FH+dj6hruu61tqXkmxcZbejzxy/N8nqoc9/urX2xnnedvh2++7g48MmFBIlzL2Gc8PcR5KktfZ9GbzedF42fZbx3tbax7uuu2cCD/349iMLvs1891rdP8Z9Dmd3dNjs3pt9fuQO3s80X+NjJ+JXdpTVdd1dXde9O8kLM1hKvdGuSd66NEc1VeMU3fAijB1d1DCpiQ9LvrmZ2aCQKK/ruq8m+XebXXzsqGzPrRojOzzxYl6zAEd4aLPPD+26ru3Ixw4+PmxCIdEX12UwOWCjA5bqQKboX4+Rfc7Qn7+xg4+3+Wth01jFCPOmkOiFuU2iw4W0vWXXfTSv1XKttX2yaSHduiMPNrfxd/htPJZ6jBM7OYVEL7TWVmcwXHSjry3VsUzRUa21584j96PZ9HWbv1jAY1459OezWmtWzLFkFBKLqrV2fGttnF9NbfQz2fTv66y+L9Jvb+vK1tozkvznoYuu7brunxfweBck2bj3aY8kH5rbnwSLTiGx2F6Z5I7W2kfnymmb//jNvU/R+Rm8xflGazK/iQZ9dEZr7b+O+r7MldEnMhhyutFvLOTB5sYUvW/oojckuaS1ts2l5K21XebO3yfm3ooDFsw+JJbC8gz2F/14krtba9dl8BYLd2UwPmdZBu+9c1iS05Ns/mus/7j5UNMZ8SdJzspgTt6rWmsXZbDcfdcMNgz/VDZdzPF7XddN4pniOzOYmvGauc/PSHJKa+3/ZPDrwK9msCT9GRm89cVhGUw93/heUBdM4BhAIbHo1m/2+YFJfmzuY3u+neQdXdd9eOJHVcNnMpjW8BsZFNBR28h+Isl/mMSDdl23obX2+iS/k+TcuYv3yGCv1yzu96Iov7Jjsf1aBsNA/3uS27JlQY3y1Qz+sXz+DJdRkqQbvAvrG5L801Yi9yd5R5LT5zHgdZzHXdd13U9nMI7oU9n+Jt17k/xBklOysEUV8KQ29JYrsOhaa7tnMA3732Swim6PDJZ0r8lgQOnfJvnnbkb/orbW7kzy7LlPf6Lruo8MXfeSDL43+2Xw/fiHDBYxTH3Je2tttwzervz7MxgxtMvcMdyV5Pau6/5x2sfAzkchwRLaViHBzsav7AAoQSEBUIJCAqAEhQRACQoJgBKssgOgBM+QAChBIQFQwlQLqbX2h621WZ3KDMAETfU1pNbarW3lvoftetCZW82s/fwHNvl85aHnLig3jfuUk5ulXB+OUa6fuaH8Dr2nll/ZAVCCQgKgBIUEQAkKCYASFBIAJSgkAEpQSACUMPV9SIcddthht95669QeA4By7EMCoL8UEgAlKCQASig3y+6xJ0bnViwf/HfDhg353Oc+l0suvSy33XZr7vrKnXnkkUeyfv36rFy5Mt+zenUO2H//POc5z8nBBx+cQw45JC8+/CXZd999t3qf833saeWqzaMa95zMWm7Wz8c42Sq5WT8nfctNa5bd8u1H6rj11lvz1re+NV/4whdGXr9mzZqsWbMmd33lK7n55ps3ue62L/xdDn7BCxbjMAHYAb0ppBtuuCGnnHJKHn300R26/YYNGyZ8RABMUi9eQ1q7dm3e9KY37XAZAVBfL54hXf6Jj+fOO+/c4vKXvPRlOe/8n8/hRxyZvffeO+vWrcuDDz6YO//p73Pbbbflqquuyg033JB169Yt/kEDMJZeFNJ1f/65LS575jOfmSuvuiYrV6588rIVK1Zk1apV+YHnfF9e9apX5Rd/8Rdz//3356KPXpy9nvGMRTxiAMbVi0kNr3nNa3LllVductlhhx0WEyAASprdSQ2tbfm1ffGLX8xVV121BEcDwDT04ld2z3ve87a47IknnsjJJ5+co48+OieffHKOOOKIHHroodl///2X4AgBWKhe/Mru+uuvz3HHHTev7H777ZdjjjkmJ554Yk4//fQ861nPWtBjAzC2HfqVXW8mNbzpnB/Jxz72sbEef9myZTnnnHPyy7/+mznwwAO3uL76rme70JcmN+vnY5xsldysn5O+5aY1qaEXryElyUc/+tGcffbZY91m/fr1ufjii/PSIw7JrX/zN1M6MgAmoTeFtNtuu+WP//iPc/XVV+fVr351li1bNu/bPvDAAzn7zNPy2GOPTfEIAViI3hTSRieddFKuuOKK3PW1b+ZPLv2/+dnzzs+xxx2fPfbYY5u3u+fuu/Pxy8b7lR8Ai6d3hbTR6tWr87rXvyG/9Z735qprrs3Xv/Vgrrn2hvzk235qq8+ebvrLGxf5KAGYr14s+56P5cuX5+hjjsnRxxyT5x/0vJx//vlbZL75rW8uwZEBMB+9eIZ0zz33ZJzVgK9+9atHXr77brtP6pAAmLBe7EM6//zzc8UVV+S8887LmWeemT333HOb+Ysuuihvectbtrj8ne98Z9797ncv6FgA2K7ZfoO+22+/PW9729ty7rnn5pRTTsnxxx+fo446Ks9+9rOzevXqrF27NnfddVcuvfTSXHDBBSPv45WvfOXiHjQA89abQtrou9/9bi6//PJcfvnlY93uyCOPnPe0BwAWXy9eQ1qoffbZJxdddNHIIa0A1FBudNCokRTrH7kzR+3/UG666aax34r8+OOPz/s/+KH8wEEHbXHdrI4xqZ7b2cbKVMuNyjontXJ9PR9D+dl9DWnZnt+fG274QB544IHceOONueWWW/JbH/ijdN99JN0T30nWr0u69ckuy5Nlu+YVx780hx9+eM4444wcccQR25zpBUANvSikjVavXp1TTz01p556at736Ye2mvvsZz+w1esAqGmneA0JgPoUEgAlKCQASujFpAYAemW236APgNmmkAAoQSEBUEIvJjWMkxuVnfSuZ7laueq77qvlRmWrn2O5heX6MqnBMyQASlBIAJSgkAAoQSEBUIJCAqAEkxoAmDSr7ADoL4UEQAkKCYASTGqQ632u2iSE6rlR2ernWG5hOZMaAGAMCgmAEhQSACUoJABKUEgAlGBSAwCTZpUdAP2lkAAoYfn2IzWtW7cuV199dW688cbcdNNNufvuu/PAAw/k4YcfztOf/vTsvvvuOeCAA/Lc5z43L37xi3PsscfmZS97WVasWLHUhw7ACL2b1LBmzZq8//3vz7t+7beSdY+OdTyrVq3Ka1/3hrzvgvdnr732Kr/redq76X0dO2duVHZWzomvY3TOpIYpuPnmm3PIIYfkXe9619hllAzK7A8v/oPc961vTeHoAFiI3vzK7lOf+lROO+20PPHEVv6XAIBe60UhbXj03px11llbLaO2Yu8s2/sF2WX3/dOevkce+usP5uGHH87Xv/713Hbbbbn2uuvz6T/9ZB544IFFPnIA5qt8IXVdl3V3X5vHH1u75ZVtlyz/Vy/P8n1euMnFK1asyIoVK7LffvvlkEMOyY/++FvyxBNP5IpP/2n+x/veuzgHDsBYyhfShgf/Id1j94+8bvn3npDlex88r/tZvnx5Xvu61+e1r3t9prmQA4AdU35Swyte8Ypcc801W1x+4oknjrwcgCU3e6vsvv3tb+f6668fed3b3/72RT4aAKap9K/sbr/99jz++OMjrzvhhBNGXt51XdavX7/d+16+vPSXDrDTKf0M6Vtb2S+0atWqrF69euR1H/zgB/O0pz1tux8A1DL1pwmfv/3uHd4x/s37Hxp5mz333HOrO5TXbf/J0ZOP07f3u5+Vx62eqzZZYdLnY5xslZyfkVq5eUxq2Ob1W1P6GdJee+018vI1a9Ys8pEAMG2lC2mfffYdefkjjzySBx98cJGPBoBpKl1Izz/44K2+3nPD9X8x8vJ///Zz03Xdkx/HH3/8NA8RgAkpXUirVq3K0ce8fOR1v/97H1rkowFgmkoXUpKc86M/NvLyz/zZlbnsY5cu8tEAMC3lJzWsX78+L3zhC3P77bdvcd3KlSvz4Q9/OGefffZWb3/CCSfkuuuu2+Jy44MApmb2JjUkybJly3LhhRdm11133eK6tWvX5pxzzskJJ5yQj3zkI7njjjvyyCOP5NFHH80999yTT37yk7n77ruX4KgBGFf5Z0gbXXLJJTnnnHOyYcOGCRyZZ0gAUzSbz5A2OvPMM/PZz342BxxwwFIfCgBTMPVnSG3lvoftetCZW82Mu+P5vvvuy3vf+95ceOGFeeihh8Y6nv322y8/ctY5efNPvDU/+EM/VH7Xs13oS5MzqaFezs9Irdw8JjXs0DOk3k0Y3WefffKe97wnP/9Lv5zP/NmVufGG6/NXt9ycb3zj3jz04IP5zne+k9133z2rVq3KgQcemIMOOigH/+CL8m9PPCkvfNGL0toOfZ8AmLLeFdJGu+++e047/YycdvoZW1w3zv8pAlBDb15DAmC2KSQASlBIAJTQm31IAPTGbO9DAmC2KSQASlBIAJRQblLDQne1T+M+q+R2tokEctPJ9eEY/Yz0MzeU9xoSAP2lkAAoQSEBUIJCAqAEhQRACSY1ADBpVtkB0F8KCYASFBIAJZjU0KPcpHehy+2cuaV87L7lqv+bUC03lPcaEgD9pZAAKEEhAVCCQgKgBIUEQAkmNQAwaVbZAdBfCgmAEhQSACWY1NCjXPVd7XL9yC3lY/ctV/3fhGq5obzXkADoL4UEQAkKCYASFBIAJSgkAEowqQGASbPKDoD+UkgAlKCQACjBpIYe5exCr5Xr6/mYxn1WyfX1nMxKbijvNSQA+kshAVCCQgKgBIUEQAkKCYASTGoAYNKssgOgvxQSACUoJABKKDepYaE7rcfJVslV34Ve/fs3K9+XxXrccbJVcrN+TvqWM6kBgJmmkAAoQSEBUIJCAqAEhQRACSY1ADBpVtkB0F8KCYASFBIAJZjUUCBnF3qt3Kyfj3GyVXKzfk76ljOpAYCZppAAKEEhAVCCQgKgBIUEQAkmNQAwaVbZAdBfCgmAEhQSACWUm9Qw353R29opXGX39qRzvo7ROV/HeLlR2b5+Lb6O+eUW8+/WXN5rSAD0l0ICoASFBEAJCgmAEhQSACWY1ADApFllB0B/KSQASlBIAJRgUoNc73N204+XG5Wtfo7lFpYzqQEAxqCQAChBIQFQgkICoASFBEAJJjUAMGlW2QHQXwoJgBIUEgAlmNQg1/ucSQ3j5UZlq59juYXlTGoAgDEoJABKUEgAlKCQAChBIQFQgkkNAEyaVXYA9JdCAqAEhQRACSY17EBuqSYDzEpuVs5HX7+OUdm+fi2zmuvr+RjKew0JgP5SSACUoJAAKEEhAVCCQgKgBJMaAJg0q+wA6C+FBEAJCgmAEspNaljoDuVxslVyO9uu8eq56rvp/Yw8xc/I0uRMagBgpikkAEpQSACUoJAAKEEhAVCCSQ0ATJpVdgD0l0ICoASFBEAJJjUUyNmFXitnUkO9nJ+RWjmTGgCYaQoJgBIUEgAlKCQASlBIAJRgUgMAk2aVHQD9pZAAKEEhAVBCuUkNC91BPY37lJMblevrNIJp3Kec3GZ5ryEB0F8KCYASFBIAJSgkAEpQSACUYFIDAJNmlR0A/aWQAChBIQFQwvLtRxam67a+Uz2Z/Hu/T+M+5eQWI2dSg1zfc1vLz5dnSACUoJAAKEEhAVCCQgKgBIUEQAkKCYASjA4CYNKMDgKgvxQSACUoJABKMDpITq5Izuggub7ntpafL8+QAChBIQFQgkICoASFBEAJCgmAEkxqAGDSTGoAoL8UEgAlKCQASpj6a0ht5b6H7XrQmVvN2IUu19ecSQ1ycqOt/fwHvIYEQH8pJABKUEgAlKCQAChBIQFQgkkNAEyaVXYA9JdCAqAEhQRACeUmNSx09/s42Sq5arutpz2RoHqu+qQGPyNP8TOyNDmTGgCYaQoJgBIUEgAlKCQASlBIAJRgUgMAk2aVHQD9pZAAKEEhAVCCSQ0Fcnah18qZ1FAv52ekVs6kBgBmmkICoASFBEAJCgmAEhQSACWY1ADApFllB0B/KSQASlBIAJRQblLDQndQj8rubLvGq+dm5Xz09esYle3r1zKrub6ej6G815AA6C+FBEAJCgmAEhQSACUoJABKMKkBgEmzyg6A/lJIAJSgkAAowaQGud7n7KYfLzcqW/0cyy0sZ1IDAIxBIQFQgkICoASFBEAJCgmAEkxqAGDSrLIDoL8UEgAlKCQASjCpQa73OZMaxsuNylY/x3ILy5nUAABjUEgAlKCQAChBIQFQgkICoASTGgCYNKvsAOgvhQRACQoJgBJMatiBnMkAC8v5OkbnZmlSg3OysFxfv46hvNeQAOgvhQRACQoJgBIUEgAlKCQASjCpAYBJs8oOgP5SSACUoJAAKKHcpIaF7lAeJ1slV333dvXv36x8XxbrccfJVsnN+jnpW86kBgBmmkICoASFBEAJCgmAEhQSACWY1ADApFllB0B/KSQASlBIAJRgUkOBnF3otXKzfj7GyVbJzfo56VvOpAYAZppCAqAEhQRACQoJgBIUEgAlmNQAwKRZZQdAfykkAEpQSACUUG5Sw3x3Rm9rp/Ck77NKbql2ocuNzvX1fEzjPqvk+npOZiU3lPcaEgD9pZAAKEEhAVCCQgKgBIUEQAkmNQAwaVbZAdBfCgmAEhQSACWY1NCj3KR3ocvtnLmlfOy+5ar/m1AtN5T3GhIA/aWQAChBIQFQgkICoASFBEAJJjUAMGlW2QHQXwoJgBIUEgAlmNTQo1z1Xe1y/cgt5WP3LVf934RquaG815AA6C+FBEAJCgmAEhQSACUoJABKMKkBgEmzyg6A/lJIAJSgkAAowaSGHuXsQpebRK4Px+hnpJ+5obzXkADoL4UEQAkKCYASFBIAJSgkAEowqQGASbPKDoD+UkgAlKCQACih3KSGhe60HidbJVd9F3r179/Otjvfz8hT/IwsTc6kBgBmmkICoASFBEAJCgmAEhQSACWY1ADApFllB0B/KSQASlBIAJRgUkOBnF3otXImNdTL+RmplTOpAYCZppAAKEEhAVCCQgKgBIUEQAkmNQAwaVbZAdBfCgmAEhQSACWUm9Sw0F3to7KzspPf1zE6V32yQrXcqOysnBNfx+jcYv7dmst7DQmA/lJIAJSgkAAoQSEBUIJCAqAEkxoAmDSr7ADoL4UEQAkKCYASTGqQ632u2iSE6rlR2ernWG5hOZMaAGAMCgmAEhQSACUoJABKUEgAlGBSAwCTZpUdAP2lkAAoQSEBUIJJDXK9z1WbhFA9Nypb/RzLLSxnUgMAjEEhAVCCQgKgBIUEQAkKCYASTGoAYNKssgOgvxQSACUoJABKMKlhB3LVdt73Led8LG1uVNY5qZXr6/kYynsNCYD+UkgAlKCQAChBIQFQgkICoASTGgCYNKvsAOgvhQRACQoJgBLKTWpY6A7lcbJVcjvbrvHquVk/H+Nkq+Rm/Zz0LWdSAwAzTSEBUIJCAqAEhQRACQoJgBIUEgAlGB0EwKRZ9g1AfykkAEpQSACUYHRQgZyxKLVys34+xslWyc36OelbzuggAGaaQgKgBIUEQAkKCYASFBIAJZjUAMCkWWUHQH8pJABKUEgAlFBuUsNCd1BP4z7l5GYp14djlOtnbijvNSQA+kshAVCCQgKgBIUEQAkKCYASFBIAJUx72ff9actXtxXfs9XMoQcfuMnnn7/97gXlpnGfcnKzlOvDMcr1M7dRt/Zbf9R13Ru3GRph2oX05SR7Jrlzag8CQDV/X66QAGC+vIYEQAkKCYASFBIAJSgkAEpQSACUoJAAKEEhAVCCQgKgBIUEQAkKCYASFBIAJSgkAEpQSACUoJAAKEEhAVCCQgKgBIUEQAkKCYASFBIAJSgkAEpQSACUoJAAKOH/AxRXHJGhW09OAAAAAElFTkSuQmCC"
                    },
                    "metadata": {
                        "image/png": {
                            "width": 210,
                            "height": 269
                        }
                    }
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('nma': conda)"
        },
        "interpreter": {
            "hash": "3e19903e646247cead5404f55ff575624523d45cf244c3f93aaf5fa10367032a"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}