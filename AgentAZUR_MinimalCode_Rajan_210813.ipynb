{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AgentAZUR_MinimalCode",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('nmarl': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "16466b8fba2a6ac6e61862ec7dae953cfc93f6c9e5771a8e7fd97f2453f609b9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\r\n",
        "\r\n",
        "## Setup\r\n",
        "\r\n",
        "Run the following 5 cells in order to set up needed functions. Don't worry about the code for now!"
      ],
      "metadata": {
        "execution": {},
        "id": "aSq3cF9wuiVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# # @title Install requirements\r\n",
        "# from IPython.display import clear_output\r\n",
        "# # @markdown we install the acme library, see [here](https://github.com/deepmind/acme) for more info\r\n",
        "\r\n",
        "# # @markdown WARNING: There may be errors and warnings reported during the installation.\r\n",
        "# # @markdown However, they should be ignored.\r\n",
        "# !apt-get install -y xvfb ffmpeg --quiet\r\n",
        "# !pip install --upgrade pip --quiet\r\n",
        "# !pip install imageio --quiet\r\n",
        "# !pip install imageio-ffmpeg\r\n",
        "# !pip install gym --quiet\r\n",
        "# !pip install enum34 --quiet\r\n",
        "# !pip install dm-env --quiet\r\n",
        "# !pip install pandas --quiet\r\n",
        "# !pip install keras-nightly==2.5.0.dev2021020510 --quiet\r\n",
        "# !pip install grpcio==1.34.0 --quiet\r\n",
        "# !pip install tensorflow --quiet\r\n",
        "# !pip install typing --quiet\r\n",
        "# !pip install einops --quiet\r\n",
        "# !pip install dm-acme --quiet\r\n",
        "# !pip install dm-acme[reverb] --quiet\r\n",
        "# !pip install dm-acme[tf] --quiet\r\n",
        "# !pip install dm-acme[envs] --quiet\r\n",
        "# !pip install dm-env --quiet\r\n",
        "\r\n",
        "# clear_output()"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "nSEnJ_6MuiVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Import modules\r\n",
        "import gym\r\n",
        "import enum\r\n",
        "import copy\r\n",
        "import time\r\n",
        "import acme\r\n",
        "import torch\r\n",
        "import base64\r\n",
        "import dm_env\r\n",
        "import IPython\r\n",
        "import imageio\r\n",
        "import warnings\r\n",
        "import itertools\r\n",
        "import collections\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow.compat.v2 as tf\r\n",
        "\r\n",
        "from acme import specs\r\n",
        "from acme import wrappers\r\n",
        "from acme.utils import tree_utils\r\n",
        "from acme.utils import loggers\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.distributions import Categorical\r\n",
        "from typing import Callable, Sequence\r\n",
        "\r\n",
        "tf.enable_v2_behavior()\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "np.set_printoptions(precision=3, suppress=1)"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "Kxmqd42fuiVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# @title Figure settings\r\n",
        "import ipywidgets as widgets       # interactive display\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\r\n",
        "mpl.rc('image', cmap='Blues')"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "pSWTYJgvuiVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# @title Helper Functions\r\n",
        "# @markdown Implement helpers for value visualisation\r\n",
        "\r\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\r\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\r\n",
        "\r\n",
        "\r\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\r\n",
        "  plt.imshow(values, interpolation=\"nearest\",\r\n",
        "             cmap=colormap, vmin=vmin, vmax=vmax)\r\n",
        "  plt.yticks([])\r\n",
        "  plt.xticks([])\r\n",
        "  plt.colorbar(ticks=[vmin, vmax])\r\n",
        "\r\n",
        "\r\n",
        "def plot_state_value(action_values, epsilon=0.1):\r\n",
        "  q = action_values\r\n",
        "  fig = plt.figure(figsize=(4, 4))\r\n",
        "  vmin = np.min(action_values)\r\n",
        "  vmax = np.max(action_values)\r\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
        "  plt.title(\"$v(s)$\")\r\n",
        "\r\n",
        "\r\n",
        "def plot_action_values(action_values, epsilon=0.1):\r\n",
        "  q = action_values\r\n",
        "  fig = plt.figure(figsize=(8, 8))\r\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\r\n",
        "  vmin = np.min(action_values)\r\n",
        "  vmax = np.max(action_values)\r\n",
        "  dif = vmax - vmin\r\n",
        "  for a in [0, 1, 2, 3]:\r\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\r\n",
        "\r\n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\r\n",
        "    action_name = map_from_action_to_name(a)\r\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\r\n",
        "\r\n",
        "  plt.subplot(3, 3, 5)\r\n",
        "  v = (1 - epsilon) * np.max(q, axis=-1) + epsilon * np.mean(q, axis=-1)\r\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\r\n",
        "  plt.title(\"$v(s)$\")\r\n",
        "\r\n",
        "\r\n",
        "def plot_stats(stats, window=10):\r\n",
        "  plt.figure(figsize=(16,4))\r\n",
        "  plt.subplot(121)\r\n",
        "  xline = range(0, len(stats.episode_lengths), window)\r\n",
        "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\r\n",
        "  plt.ylabel('Episode Length')\r\n",
        "  plt.xlabel('Episode Count')\r\n",
        "  plt.subplot(122)\r\n",
        "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\r\n",
        "  plt.ylabel('Episode Return')\r\n",
        "  plt.xlabel('Episode Count')"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "NfmoICzbuiVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# @title Helper functions\r\n",
        "def smooth(x, window=10):\r\n",
        "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "VIaPcoDHuiVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set random seed, device"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# @title Set random seed\r\n",
        "\r\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\r\n",
        "\r\n",
        "# for DL its critical to set the random seed so that students can have a\r\n",
        "# baseline to compare their results to expected results.\r\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\r\n",
        "\r\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\r\n",
        "import random\r\n",
        "import torch\r\n",
        "\r\n",
        "def set_seed(seed=None, seed_torch=True):\r\n",
        "  if seed is None:\r\n",
        "    seed = np.random.choice(2 ** 32)\r\n",
        "  random.seed(seed)\r\n",
        "  np.random.seed(seed)\r\n",
        "  if seed_torch:\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed_all(seed)\r\n",
        "    torch.cuda.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "  print(f'Random seed {seed} has been set.')\r\n",
        "\r\n",
        "\r\n",
        "# In case that `DataLoader` is used\r\n",
        "def seed_worker(worker_id):\r\n",
        "  worker_seed = torch.initial_seed() % 2**32\r\n",
        "  np.random.seed(worker_seed)\r\n",
        "  random.seed(worker_seed)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "cfsXcrpPuiVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\r\n",
        "# especially if torch modules used.\r\n",
        "\r\n",
        "# inform the user if the notebook uses GPU or CPU.\r\n",
        "\r\n",
        "def set_device():\r\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "  if device != \"cuda\":\r\n",
        "    print(\"WARNING: For this notebook to perform best, \"\r\n",
        "        \"if possible, in the menu under `Runtime` -> \"\r\n",
        "        \"`Change runtime type.`  select `GPU` \")\r\n",
        "  else:\r\n",
        "    print(\"GPU is enabled in this notebook.\")\r\n",
        "\r\n",
        "  return device"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ILGZfK5fuiVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "SEED = 2021\r\n",
        "set_seed(seed=SEED)\r\n",
        "DEVICE = set_device()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ]
        }
      ],
      "metadata": {
        "execution": {},
        "id": "oGM9gCX5uiVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef42b7e-8627-43d9-e3da-44f51dff0916"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acme: a research framework for reinforcement learning, Set up gridworld"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acme** is a library of reinforcement learning (RL) agents and agent building blocks by Google DeepMind. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.\r\n",
        "\r\n",
        "For more information see [github repository](https://github.com/deepmind/acme)."
      ],
      "metadata": {
        "execution": {},
        "id": "q8DsUg9SuiVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# @title Implement GridWorld { form-width: \"30%\" }\r\n",
        "# @markdown *Double-click* to inspect the contents of this cell.\r\n",
        "\r\n",
        "class ObservationType(enum.IntEnum):\r\n",
        "  STATE_INDEX = enum.auto()\r\n",
        "  AGENT_ONEHOT = enum.auto()\r\n",
        "  GRID = enum.auto()\r\n",
        "  AGENT_GOAL_POS = enum.auto()\r\n",
        "\r\n",
        "\r\n",
        "class GridWorld(dm_env.Environment):\r\n",
        "\r\n",
        "  def __init__(self,\r\n",
        "               layout,\r\n",
        "               start_state,\r\n",
        "               goal_state=None,\r\n",
        "               observation_type=ObservationType.STATE_INDEX,\r\n",
        "               discount=0.9,\r\n",
        "               penalty_for_walls=-5,\r\n",
        "               reward_goal=10,\r\n",
        "               max_episode_length=None,\r\n",
        "               randomize_goals=False):\r\n",
        "    \"\"\"Build a grid environment.\r\n",
        "\r\n",
        "    Simple gridworld defined by a map layout, a start and a goal state.\r\n",
        "\r\n",
        "    Layout should be a NxN grid, containing:\r\n",
        "      * 0: empty\r\n",
        "      * -1: wall\r\n",
        "      * Any other positive value: value indicates reward; episode will terminate\r\n",
        "\r\n",
        "    Args:\r\n",
        "      layout: NxN array of numbers, indicating the layout of the environment.\r\n",
        "      start_state: Tuple (y, x) of starting location.\r\n",
        "      goal_state: Optional tuple (y, x) of goal location. Will be randomly\r\n",
        "        sampled once if None.\r\n",
        "      observation_type: Enum observation type to use. One of:\r\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\r\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\r\n",
        "          agent is and 0 elsewhere.\r\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\r\n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the\r\n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\r\n",
        "          (1 if goal, 0 otherwise)\r\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\r\n",
        "          (agent_y, agent_x, goal_y, goal_x)\r\n",
        "      discount: Discounting factor included in all Timesteps.\r\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\r\n",
        "      reward_goal: Reward added when finding the goal (should be positive).\r\n",
        "      max_episode_length: If set, will terminate an episode after this many\r\n",
        "        steps.\r\n",
        "      randomize_goals: If true, randomize goal at every episode.\r\n",
        "    \"\"\"\r\n",
        "    if observation_type not in ObservationType:\r\n",
        "      raise ValueError('observation_type should be a ObservationType instace.')\r\n",
        "    self._layout = np.array(layout)\r\n",
        "    self._start_state = start_state\r\n",
        "    self._state = self._start_state\r\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\r\n",
        "    self._discount = discount\r\n",
        "    self._penalty_for_walls = penalty_for_walls\r\n",
        "    self._reward_goal = reward_goal\r\n",
        "    self._observation_type = observation_type\r\n",
        "    self._layout_dims = self._layout.shape\r\n",
        "    self._max_episode_length = max_episode_length\r\n",
        "    self._num_episode_steps = 0\r\n",
        "    self._randomize_goals = randomize_goals\r\n",
        "    if goal_state is None:\r\n",
        "      # Randomly sample goal_state if not provided\r\n",
        "      goal_state = self._sample_goal()\r\n",
        "    self.goal_state = goal_state\r\n",
        "\r\n",
        "  def _sample_goal(self):\r\n",
        "    \"\"\"Randomly sample reachable non-starting state.\"\"\"\r\n",
        "    # Sample a new goal\r\n",
        "    n = 0\r\n",
        "    max_tries = 1e5\r\n",
        "    while n < max_tries:\r\n",
        "      goal_state = tuple(np.random.randint(d) for d in self._layout_dims)\r\n",
        "      if goal_state != self._state and self._layout[goal_state] == 0:\r\n",
        "        # Reachable state found!\r\n",
        "        return goal_state\r\n",
        "      n += 1\r\n",
        "    raise ValueError('Failed to sample a goal state.')\r\n",
        "\r\n",
        "  @property\r\n",
        "  def layout(self):\r\n",
        "    return self._layout\r\n",
        "\r\n",
        "  @property\r\n",
        "  def number_of_states(self):\r\n",
        "    return self._number_of_states\r\n",
        "\r\n",
        "  @property\r\n",
        "  def goal_state(self):\r\n",
        "    return self._goal_state\r\n",
        "\r\n",
        "  @property\r\n",
        "  def start_state(self):\r\n",
        "    return self._start_state\r\n",
        "\r\n",
        "  @property\r\n",
        "  def state(self):\r\n",
        "    return self._state\r\n",
        "\r\n",
        "  def set_state(self, x, y):\r\n",
        "    self._state = (y, x)\r\n",
        "\r\n",
        "  @goal_state.setter\r\n",
        "  def goal_state(self, new_goal):\r\n",
        "    if new_goal == self._state or self._layout[new_goal] < 0:\r\n",
        "      raise ValueError('This is not a valid goal!')\r\n",
        "    # Zero out any other goal\r\n",
        "    self._layout[self._layout > 0] = 0\r\n",
        "    # Setup new goal location\r\n",
        "    self._layout[new_goal] = self._reward_goal\r\n",
        "    self._goal_state = new_goal\r\n",
        "\r\n",
        "  def observation_spec(self):\r\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\r\n",
        "      return specs.Array(\r\n",
        "          shape=self._layout_dims,\r\n",
        "          dtype=np.float32,\r\n",
        "          name='observation_agent_onehot')\r\n",
        "    elif self._observation_type is ObservationType.GRID:\r\n",
        "      return specs.Array(\r\n",
        "          shape=self._layout_dims + (3,),\r\n",
        "          dtype=np.float32,\r\n",
        "          name='observation_grid')\r\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\r\n",
        "      return specs.Array(\r\n",
        "          shape=(4,), dtype=np.float32, name='observation_agent_goal_pos')\r\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\r\n",
        "      return specs.DiscreteArray(\r\n",
        "          self._number_of_states, dtype=int, name='observation_state_index')\r\n",
        "\r\n",
        "  def action_spec(self):\r\n",
        "    return specs.DiscreteArray(4, dtype=int, name='action')\r\n",
        "\r\n",
        "  def get_obs(self):\r\n",
        "    if self._observation_type is ObservationType.AGENT_ONEHOT:\r\n",
        "      obs = np.zeros(self._layout.shape, dtype=np.float32)\r\n",
        "      # Place agent\r\n",
        "      obs[self._state] = 1\r\n",
        "      return obs\r\n",
        "    elif self._observation_type is ObservationType.GRID:\r\n",
        "      obs = np.zeros(self._layout.shape + (3,), dtype=np.float32)\r\n",
        "      obs[..., 0] = self._layout < 0\r\n",
        "      obs[self._state[0], self._state[1], 1] = 1\r\n",
        "      obs[self._goal_state[0], self._goal_state[1], 2] = 1\r\n",
        "      return obs\r\n",
        "    elif self._observation_type is ObservationType.AGENT_GOAL_POS:\r\n",
        "      return np.array(self._state + self._goal_state, dtype=np.float32)\r\n",
        "    elif self._observation_type is ObservationType.STATE_INDEX:\r\n",
        "      y, x = self._state\r\n",
        "      return y * self._layout.shape[1] + x\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    self._state = self._start_state\r\n",
        "    self._num_episode_steps = 0\r\n",
        "    if self._randomize_goals:\r\n",
        "      self.goal_state = self._sample_goal()\r\n",
        "    return dm_env.TimeStep(\r\n",
        "        step_type=dm_env.StepType.FIRST,\r\n",
        "        reward=None,\r\n",
        "        discount=None,\r\n",
        "        observation=self.get_obs())\r\n",
        "\r\n",
        "  def step(self, action):\r\n",
        "    y, x = self._state\r\n",
        "\r\n",
        "    if action == 0:  # up\r\n",
        "      new_state = (y - 1, x)\r\n",
        "    elif action == 1:  # right\r\n",
        "      new_state = (y, x + 1)\r\n",
        "    elif action == 2:  # down\r\n",
        "      new_state = (y + 1, x)\r\n",
        "    elif action == 3:  # left\r\n",
        "      new_state = (y, x - 1)\r\n",
        "    else:\r\n",
        "      raise ValueError(\r\n",
        "          'Invalid action: {} is not 0, 1, 2, or 3.'.format(action))\r\n",
        "\r\n",
        "    new_y, new_x = new_state\r\n",
        "    step_type = dm_env.StepType.MID\r\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\r\n",
        "      reward = self._penalty_for_walls\r\n",
        "      discount = self._discount\r\n",
        "      new_state = (y, x)\r\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\r\n",
        "      reward = 0.\r\n",
        "      discount = self._discount\r\n",
        "    else:  # a goal\r\n",
        "      reward = self._layout[new_y, new_x]\r\n",
        "      discount = 0.\r\n",
        "      new_state = self._start_state\r\n",
        "      step_type = dm_env.StepType.LAST\r\n",
        "\r\n",
        "    self._state = new_state\r\n",
        "    self._num_episode_steps += 1\r\n",
        "    if (self._max_episode_length is not None and\r\n",
        "        self._num_episode_steps >= self._max_episode_length):\r\n",
        "      step_type = dm_env.StepType.LAST\r\n",
        "    return dm_env.TimeStep(\r\n",
        "        step_type=step_type,\r\n",
        "        reward=np.float32(reward),\r\n",
        "        discount=discount,\r\n",
        "        observation=self.get_obs())\r\n",
        "\r\n",
        "  def plot_grid(self, add_start=True):\r\n",
        "    plt.figure(figsize=(4, 4))\r\n",
        "    plt.imshow(self._layout <= -1, interpolation='nearest')\r\n",
        "    ax = plt.gca()\r\n",
        "    ax.grid(0)\r\n",
        "    plt.xticks([])\r\n",
        "    plt.yticks([])\r\n",
        "    # Add start/goal\r\n",
        "    if add_start:\r\n",
        "      plt.text(\r\n",
        "          self._start_state[1],\r\n",
        "          self._start_state[0],\r\n",
        "          r'$\\mathbf{S}$',\r\n",
        "          fontsize=16,\r\n",
        "          ha='center',\r\n",
        "          va='center')\r\n",
        "    plt.text(\r\n",
        "        self._goal_state[1],\r\n",
        "        self._goal_state[0],\r\n",
        "        r'$\\mathbf{G}$',\r\n",
        "        fontsize=16,\r\n",
        "        ha='center',\r\n",
        "        va='center')\r\n",
        "    h, w = self._layout.shape\r\n",
        "    for y in range(h - 1):\r\n",
        "      plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-w', lw=2)\r\n",
        "    for x in range(w - 1):\r\n",
        "      plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-w', lw=2)\r\n",
        "\r\n",
        "  def plot_state(self, return_rgb=False):\r\n",
        "    self.plot_grid(add_start=False)\r\n",
        "    # Add the agent location\r\n",
        "    plt.text(\r\n",
        "        self._state[1],\r\n",
        "        self._state[0],\r\n",
        "        u'😃',\r\n",
        "        # fontname='symbola',\r\n",
        "        fontsize=18,\r\n",
        "        ha='center',\r\n",
        "        va='center',\r\n",
        "    )\r\n",
        "    if return_rgb:\r\n",
        "      fig = plt.gcf()\r\n",
        "      plt.axis('tight')\r\n",
        "      plt.subplots_adjust(0, 0, 1, 1, 0, 0)\r\n",
        "      fig.canvas.draw()\r\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\r\n",
        "      w, h = fig.canvas.get_width_height()\r\n",
        "      data = data.reshape((h, w, 3))\r\n",
        "      plt.close(fig)\r\n",
        "      return data\r\n",
        "\r\n",
        "  def plot_policy(self, policy):\r\n",
        "    action_names = [\r\n",
        "        r'$\\uparrow$', r'$\\rightarrow$', r'$\\downarrow$', r'$\\leftarrow$'\r\n",
        "    ]\r\n",
        "    self.plot_grid()\r\n",
        "    plt.title('Policy Visualization')\r\n",
        "    h, w = self._layout.shape\r\n",
        "    for y in range(h):\r\n",
        "      for x in range(w):\r\n",
        "        # if ((y, x) != self._start_state) and ((y, x) != self._goal_state):\r\n",
        "        if (y, x) != self._goal_state:\r\n",
        "          action_name = action_names[policy[y, x]]\r\n",
        "          plt.text(x, y, action_name, ha='center', va='center')\r\n",
        "\r\n",
        "  def plot_greedy_policy(self, q):\r\n",
        "    greedy_actions = np.argmax(q, axis=2)\r\n",
        "    self.plot_policy(greedy_actions)\r\n",
        "\r\n",
        "\r\n",
        "def build_gridworld_task(task,\r\n",
        "                         discount=0.9,\r\n",
        "                         penalty_for_walls=-5,\r\n",
        "                         observation_type=ObservationType.STATE_INDEX,\r\n",
        "                         max_episode_length=200):\r\n",
        "  \"\"\"Construct a particular Gridworld layout with start/goal states.\r\n",
        "\r\n",
        "  Args:\r\n",
        "      task: string name of the task to use. One of {'simple', 'obstacle',\r\n",
        "        'random_goal'}.\r\n",
        "      discount: Discounting factor included in all Timesteps.\r\n",
        "      penalty_for_walls: Reward added when hitting a wall (should be negative).\r\n",
        "      observation_type: Enum observation type to use. One of:\r\n",
        "        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.\r\n",
        "        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the\r\n",
        "          agent is and 0 elsewhere.\r\n",
        "        * ObservationType.GRID: NxNx3 float32 grid of feature channels.\r\n",
        "          First channel contains walls (1 if wall, 0 otherwise), second the\r\n",
        "          agent position (1 if agent, 0 otherwise) and third goal position\r\n",
        "          (1 if goal, 0 otherwise)\r\n",
        "        * ObservationType.AGENT_GOAL_POS: float32 tuple with\r\n",
        "          (agent_y, agent_x, goal_y, goal_x).\r\n",
        "      max_episode_length: If set, will terminate an episode after this many\r\n",
        "        steps.\r\n",
        "  \"\"\"\r\n",
        "  tasks_specifications = {\r\n",
        "      'simple': {\r\n",
        "          'layout': [\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "          ],\r\n",
        "          'start_state': (2, 2),\r\n",
        "          'goal_state': (7, 2)\r\n",
        "      },\r\n",
        "      'obstacle': {\r\n",
        "          'layout': [\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, -1, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "          ],\r\n",
        "          'start_state': (2, 2),\r\n",
        "          'goal_state': (2, 8)\r\n",
        "      },\r\n",
        "      'random_goal': {\r\n",
        "          'layout': [\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, -1, -1, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, 0, 0, 0, 0, 0, 0, 0, 0, -1],\r\n",
        "              [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\r\n",
        "          ],\r\n",
        "          'start_state': (2, 2),\r\n",
        "          # 'randomize_goals': True\r\n",
        "      },\r\n",
        "  }\r\n",
        "  return GridWorld(\r\n",
        "      discount=discount,\r\n",
        "      penalty_for_walls=penalty_for_walls,\r\n",
        "      observation_type=observation_type,\r\n",
        "      max_episode_length=max_episode_length,\r\n",
        "      **tasks_specifications[task])\r\n",
        "\r\n",
        "\r\n",
        "def setup_environment(environment):\r\n",
        "  \"\"\"Returns the environment and its spec.\"\"\"\r\n",
        "\r\n",
        "  # Make sure the environment outputs single-precision floats.\r\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\r\n",
        "\r\n",
        "  # Grab the spec of the environment.\r\n",
        "  environment_spec = specs.make_environment_spec(environment)\r\n",
        "\r\n",
        "  return environment, environment_spec"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "LbICvlWUuiVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play around with gridworlds"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use two distinct tabular GridWorlds:\r\n",
        "* `simple` where the goal is at the bottom left of the grid, little navigation required.\r\n",
        "* `obstacle` where the goal is behind an obstacle the agent must avoid.\r\n",
        "\r\n",
        "You can visualize the grid worlds by running the cell below. \r\n",
        "\r\n",
        "Note that **S** indicates the start state and **G** indicates the goal. \r\n"
      ],
      "metadata": {
        "execution": {},
        "id": "6Qmnw5IOuiVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# Visualise GridWorlds\r\n",
        "\r\n",
        "# Instantiate two tabular environments, a simple task, and one that involves\r\n",
        "# the avoidance of an obstacle.\r\n",
        "simple_grid = build_gridworld_task(\r\n",
        "    task='simple', observation_type=ObservationType.GRID)\r\n",
        "obstacle_grid = build_gridworld_task(\r\n",
        "    task='obstacle', observation_type=ObservationType.GRID)\r\n",
        "\r\n",
        "# Plot them.\r\n",
        "simple_grid.plot_grid()\r\n",
        "plt.title('Simple')\r\n",
        "\r\n",
        "obstacle_grid.plot_grid()\r\n",
        "plt.title('Obstacle')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Obstacle')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "image/png": {
              "width": 270,
              "height": 264
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "image/png": {
              "width": 270,
              "height": 264
            }
          }
        }
      ],
      "metadata": {
        "execution": {},
        "id": "h9xj28j4uiVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this environment, the agent has four possible  <font color='blue'>**actions**</font>: `up`, `right`, `down`, and `left`.  The <font color='green'>**reward**</font> is `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise. The episode ends when the agent reaches the goal, and otherwise continues. The **discount** on continuing steps, is $\\gamma = 0.9$. \n",
        "\n",
        "Before we start building an agent to interact with this environment, let's first look at the types of objects the environment either returns (e.g., <font color='redorange'>**observations**</font>) or consumes (e.g., <font color='blue'>**actions**</font>). The `environment_spec` will show you the form of the <font color='redorange'>**observations**</font>, <font color='green'>**rewards**</font> and **discounts** that the environment exposes and the form of the <font color='blue'>**actions**</font> that can be taken.\n"
      ],
      "metadata": {
        "execution": {},
        "id": "mnODEICLuiVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# @title Look at environment_spec { form-width: \"30%\" }\r\n",
        "\r\n",
        "# Note: setup_environment is implemented in the same cell as GridWorld.\r\n",
        "environment, environment_spec = setup_environment(simple_grid)\r\n",
        "\r\n",
        "print('actions:\\n', environment_spec.actions, '\\n')\r\n",
        "print('observations:\\n', environment_spec.observations, '\\n')\r\n",
        "print('rewards:\\n', environment_spec.rewards, '\\n')\r\n",
        "print('discounts:\\n', environment_spec.discounts, '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions:\n",
            " DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=3, num_values=4) \n",
            "\n",
            "observations:\n",
            " Array(shape=(9, 10, 3), dtype=dtype('float32'), name='observation_grid') \n",
            "\n",
            "rewards:\n",
            " Array(shape=(), dtype=dtype('float32'), name='reward') \n",
            "\n",
            "discounts:\n",
            " BoundedArray(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0) \n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "WOAsWtmZuiVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We first set the environment to its initial state by calling the `reset()` method which returns the first observation and resets the agent to the starting location.\n"
      ],
      "metadata": {
        "execution": {},
        "id": "UeFZBERRuiVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "environment.reset()\r\n",
        "environment.plot_state()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "image/png": {
              "width": 270,
              "height": 244
            }
          }
        }
      ],
      "metadata": {
        "execution": {},
        "id": "LZwLrDeYuiVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to take an action to interact with the environment. We do this by passing a valid action to the `dm_env.Environment.step()` method which returns a `dm_env.TimeStep` namedtuple with fields `(step_type, reward, discount, observation)`.\n",
        "\n",
        "Let's take an action and visualise the resulting state of the grid-world. (You'll need to rerun the cell if you pick a new action.)"
      ],
      "metadata": {
        "execution": {},
        "id": "fSBlGV8ruiVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# @title Pick an action and see the state changing\r\n",
        "action = \"left\" #@param [\"up\", \"right\", \"down\", \"left\"] {type:\"string\"}\r\n",
        "\r\n",
        "action_int = {'up': 0,\r\n",
        "              'right': 1,\r\n",
        "              'down': 2,\r\n",
        "              'left':3 }\r\n",
        "action = int(action_int[action])\r\n",
        "timestep = environment.step(action)  # pytype: dm_env.TimeStep\r\n",
        "environment.plot_state()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "image/png": {
              "width": 270,
              "height": 244
            }
          }
        }
      ],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "v_VfT_xKuiVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loops, evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# @title Run loop  { form-width: \"30%\" }\r\n",
        "# @markdown This function runs an agent in the environment for a number of\r\n",
        "# @markdown episodes, allowing it to learn.\r\n",
        "\r\n",
        "# @markdown *Double-click* to inspect the `run_loop` function.\r\n",
        "\r\n",
        "\r\n",
        "def run_loop(environment,\r\n",
        "             agent,\r\n",
        "             num_episodes=None,\r\n",
        "             num_steps=None,\r\n",
        "             logger_time_delta=1.,\r\n",
        "             label='training_loop',\r\n",
        "             log_loss=False,\r\n",
        "             ):\r\n",
        "  \"\"\"Perform the run loop.\r\n",
        "\r\n",
        "  We are following the Acme run loop.\r\n",
        "\r\n",
        "  Run the environment loop for `num_episodes` episodes. Each episode is itself\r\n",
        "  a loop which interacts first with the environment to get an observation and\r\n",
        "  then give that observation to the agent in order to retrieve an action. Upon\r\n",
        "  termination of an episode a new episode will be started. If the number of\r\n",
        "  episodes is not given then this will interact with the environment\r\n",
        "  infinitely.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    environment: dm_env used to generate trajectories.\r\n",
        "    agent: acme.Actor for selecting actions in the run loop.\r\n",
        "    num_steps: number of steps to run the loop for. If `None` (default), runs\r\n",
        "      without limit.\r\n",
        "    num_episodes: number of episodes to run the loop for. If `None` (default),\r\n",
        "      runs without limit.\r\n",
        "    logger_time_delta: time interval (in seconds) between consecutive logging\r\n",
        "      steps.\r\n",
        "    label: optional label used at logging steps.\r\n",
        "  \"\"\"\r\n",
        "  logger = loggers.TerminalLogger(label=label, time_delta=logger_time_delta)\r\n",
        "  iterator = range(num_episodes) if num_episodes else itertools.count()\r\n",
        "  all_returns = []\r\n",
        "\r\n",
        "  num_total_steps = 0\r\n",
        "  for episode in iterator:\r\n",
        "    # Reset any counts and start the environment.\r\n",
        "    start_time = time.time()\r\n",
        "    episode_steps = 0\r\n",
        "    episode_return = 0\r\n",
        "    episode_loss = 0\r\n",
        "\r\n",
        "    timestep = environment.reset()\r\n",
        "\r\n",
        "    # Make the first observation.\r\n",
        "    agent.observe_first(timestep)\r\n",
        "\r\n",
        "    # Run an episode.\r\n",
        "    while not timestep.last():\r\n",
        "      # Generate an action from the agent's policy and step the environment.\r\n",
        "      action = agent.select_action(timestep.observation)\r\n",
        "      timestep = environment.step(action)\r\n",
        "\r\n",
        "      # Have the agent observe the timestep and let the agent update itself.\r\n",
        "      agent.observe(action, next_timestep=timestep)\r\n",
        "      agent.update()\r\n",
        "\r\n",
        "      # Book-keeping.\r\n",
        "      episode_steps += 1\r\n",
        "      num_total_steps += 1\r\n",
        "      episode_return += timestep.reward\r\n",
        "\r\n",
        "      if log_loss:\r\n",
        "        episode_loss += agent.last_loss\r\n",
        "\r\n",
        "      if num_steps is not None and num_total_steps >= num_steps:\r\n",
        "        break\r\n",
        "\r\n",
        "    # Collect the results and combine with counts.\r\n",
        "    steps_per_second = episode_steps / (time.time() - start_time)\r\n",
        "    result = {\r\n",
        "        'episode': episode,\r\n",
        "        'episode_length': episode_steps,\r\n",
        "        'episode_return': episode_return,\r\n",
        "    }\r\n",
        "    if log_loss:\r\n",
        "      result['loss_avg'] = episode_loss/episode_steps\r\n",
        "\r\n",
        "    all_returns.append(episode_return)\r\n",
        "\r\n",
        "    # Log the given results.\r\n",
        "    logger.write(result)\r\n",
        "\r\n",
        "    if num_steps is not None and num_total_steps >= num_steps:\r\n",
        "      break\r\n",
        "  return all_returns"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "xXxgHPHBuiVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# @title Implement the evaluation loop { form-width: \"30%\" }\r\n",
        "# @markdown This function runs the agent in the environment for a number of\r\n",
        "# @markdown episodes, without allowing it to learn, in order to evaluate it.\r\n",
        "\r\n",
        "# @markdown *Double-click* to inspect the `evaluate` function.\r\n",
        "\r\n",
        "def evaluate(environment: dm_env.Environment,\r\n",
        "             agent: acme.Actor,\r\n",
        "             evaluation_episodes: int):\r\n",
        "  frames = []\r\n",
        "\r\n",
        "  for episode in range(evaluation_episodes):\r\n",
        "    timestep = environment.reset()\r\n",
        "    episode_return = 0\r\n",
        "    steps = 0\r\n",
        "    while not timestep.last():\r\n",
        "      frames.append(environment.plot_state(return_rgb=True))\r\n",
        "\r\n",
        "      action = agent.select_action(timestep.observation)\r\n",
        "      timestep = environment.step(action)\r\n",
        "      steps += 1\r\n",
        "      episode_return += timestep.reward\r\n",
        "    print(\r\n",
        "        f'Episode {episode} ended with reward {episode_return} in {steps} steps'\r\n",
        "    )\r\n",
        "  return frames\r\n",
        "\r\n",
        "def display_video(frames: Sequence[np.ndarray],\r\n",
        "                  filename: str = 'temp.mp4',\r\n",
        "                  frame_rate: int = 12):\r\n",
        "  \"\"\"Save and display video.\"\"\"\r\n",
        "  # Write the frames to a video.\r\n",
        "  with imageio.get_writer(filename, fps=frame_rate) as video:\r\n",
        "    for frame in frames:\r\n",
        "      video.append_data(frame)\r\n",
        "\r\n",
        "  # Read video and display the video.\r\n",
        "  video = open(filename, 'rb').read()\r\n",
        "  b64_video = base64.b64encode(video)\r\n",
        "  video_tag = ('<video  width=\"320\" height=\"240\" controls alt=\"test\" '\r\n",
        "               'src=\"data:video/mp4;base64,{0}\">').format(b64_video.decode())\r\n",
        "  return IPython.display.HTML(video_tag)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "hdrY7-ySuiVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code random, SARSA, policy eval agents"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Exercise 2.1: Random Agent\n",
        "\n",
        "Below is a partially complete implemention of an agent that follows a random (non-learning) policy. Fill in the ```select_action``` method.\n",
        "\n",
        "The ```select_action``` method should return a random **integer** between 0 and ```self._num_actions``` (not a tensor or an array!)"
      ],
      "metadata": {
        "execution": {},
        "id": "2D11-Hf4uiVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RandomAgent(acme.Actor):\r\n",
        "\r\n",
        "  def __init__(self, environment_spec):\r\n",
        "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\r\n",
        "    self._num_actions = environment_spec.actions.num_values\r\n",
        "\r\n",
        "  def select_action(self, observation):\r\n",
        "    \"\"\"Selects an action uniformly at random.\"\"\"\r\n",
        "    #################################################\r\n",
        "    # Fill in missing code below (...),\r\n",
        "    # then remove or comment the line below to test your implementation\r\n",
        "    raise NotImplementedError(\"Student exercise: complete the select action method\")\r\n",
        "    #################################################\r\n",
        "    # TODO return a random integer beween 0 and self._num_actions.\r\n",
        "    # HINT: see the reference for how to sample a random integer in numpy:\r\n",
        "    #   https://numpy.org/doc/1.16/reference/routines.random.html\r\n",
        "    action = ...\r\n",
        "    return action\r\n",
        "\r\n",
        "  def observe_first(self, timestep):\r\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\r\n",
        "    pass\r\n",
        "\r\n",
        "  def observe(self, action, next_timestep):\r\n",
        "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\r\n",
        "    pass\r\n",
        "\r\n",
        "  def update(self):\r\n",
        "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\r\n",
        "    pass"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "OjOAT5_7uiVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_7eaa84d6.py)\n",
        "\n"
      ],
      "metadata": {
        "execution": {},
        "id": "Bkod1o2WuiVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Visualisation of a random agent in GridWorld { form-width: \"30%\" }\r\n",
        "\r\n",
        "# Create the agent by giving it the action space specification.\r\n",
        "agent = RandomAgent(environment_spec)\r\n",
        "\r\n",
        "# Run the agent in the evaluation loop, which returns the frames.\r\n",
        "frames = evaluate(environment, agent, evaluation_episodes=1)\r\n",
        "\r\n",
        "# Visualize the random agent's episode.\r\n",
        "display_video(frames)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "IV7egAXFuiVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Exercise 4.1 Policy Evaluation Agent"
      ],
      "metadata": {
        "execution": {},
        "id": "TwrKnBMXuiVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tabular agents implement a function `q_values()` returning a matrix of Q values\n",
        "of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "In this section, we will implement a `PolicyEvalAgent` as an ACME actor: given an `evaluation_policy` $\\pi_e$ and a `behaviour_policy` $\\pi_b$, it will use the `behaviour_policy` to choose actions, and it will use the corresponding trajectory data to evaluate the `evaluation_policy` (i.e. compute the Q-values as if you were following the `evaluation_policy`). \n",
        "\n",
        "Algorithm:\n",
        "\n",
        "**Initialize** $Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s}$ ∈ $\\mathcal{\\color{red}S}$ and $\\color{blue}a$ ∈ $\\mathcal{\\color{blue}A}(\\color{red}s)$\n",
        "\n",
        "**Loop forever**:\n",
        "\n",
        "1. $\\color{red}{s} \\gets{}$current (nonterminal) state\n",
        " \n",
        "2. $\\color{blue}{a} \\gets{} \\text{behaviour_policy }\\pi_b(\\color{red}s)$\n",
        " \n",
        "3. Take action $\\color{blue}{a}$; observe resulting reward $\\color{green}{r}$, discount $\\gamma$, and state, $\\color{red}{s'}$\n",
        "\n",
        "4. Compute TD-error: $\\delta = \\color{green}R + \\gamma Q(\\color{red}{s'}, \\underbrace{\\pi_e(\\color{red}{s'}}_{\\color{blue}{a'}})) − Q(\\color{red}s, \\color{blue}a)$\n",
        "\n",
        "4. Update Q-value with a small $\\alpha$ step: $Q(\\color{red}s, \\color{blue}a) \\gets Q(\\color{red}s, \\color{blue}a) + \\alpha \\delta$\n",
        "\n",
        "\n",
        "We will use a uniform `random policy` as our `evaluation policy` here, but you could replace this with any policy you want, such as a greedy one."
      ],
      "metadata": {
        "execution": {},
        "id": "as7R2N-zuiVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Uniform random policy\r\n",
        "def random_policy(q):\r\n",
        "  return np.random.randint(4)"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "dleQpBZPuiVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PolicyEvalAgent(acme.Actor):\r\n",
        "\r\n",
        "  def __init__(self, environment_spec, evaluated_policy,\r\n",
        "               behaviour_policy=random_policy, step_size=0.1):\r\n",
        "\r\n",
        "    self._state = None\r\n",
        "    # Get number of states and actions from the environment spec.\r\n",
        "    self._number_of_states = environment_spec.observations.num_values\r\n",
        "    self._number_of_actions = environment_spec.actions.num_values\r\n",
        "    self._step_size = step_size\r\n",
        "    self._behaviour_policy = behaviour_policy\r\n",
        "    self._evaluated_policy = evaluated_policy\r\n",
        "    #################################################\r\n",
        "    # Fill in missing code below (...),\r\n",
        "    # then remove or comment the line below to test your implementation\r\n",
        "    raise NotImplementedError(\"Initialize your Q-values!\")\r\n",
        "    #################################################\r\n",
        "    # TODO Initialize the Q-values to be all zeros.\r\n",
        "    # (Note: can also be random, but we use zeros here for reproducibility)\r\n",
        "    # HINT: This is a table of state and action pairs, so needs to be a 2-D\r\n",
        "    #   array. See the reference for how to create this in numpy:\r\n",
        "    #   https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\r\n",
        "    self._q = ...\r\n",
        "    self._action = None\r\n",
        "    self._next_state = None\r\n",
        "\r\n",
        "  @property\r\n",
        "  def q_values(self):\r\n",
        "    # return the Q values\r\n",
        "    return self._q\r\n",
        "\r\n",
        "  def select_action(self, observation):\r\n",
        "    # Select an action\r\n",
        "    return self._behaviour_policy(self._q[observation])\r\n",
        "\r\n",
        "  def observe_first(self, timestep):\r\n",
        "    self._state = timestep.observation\r\n",
        "\r\n",
        "  def observe(self, action, next_timestep):\r\n",
        "    s = self._state\r\n",
        "    a = action\r\n",
        "    r = next_timestep.reward\r\n",
        "    g = next_timestep.discount\r\n",
        "    next_s = next_timestep.observation\r\n",
        "\r\n",
        "    # Compute TD-Error.\r\n",
        "    self._action = a\r\n",
        "    self._next_state = next_s\r\n",
        "    #################################################\r\n",
        "    # Fill in missing code below (...),\r\n",
        "    # then remove or comment the line below to test your implementation\r\n",
        "    raise NotImplementedError(\"Need to select the next action\")\r\n",
        "    #################################################\r\n",
        "    # TODO Select the next action from the evaluation policy\r\n",
        "    # HINT: Refer to step 4 of the algorithm above.\r\n",
        "    next_a = ...\r\n",
        "    self._td_error = r + g * self._q[next_s, next_a] - self._q[s, a]\r\n",
        "\r\n",
        "  def update(self):\r\n",
        "    # Updates\r\n",
        "    s = self._state\r\n",
        "    a = self._action\r\n",
        "    # Q-value table update.\r\n",
        "    self._q[s, a] += self._step_size * self._td_error\r\n",
        "    # Update the state\r\n",
        "    self._state = self._next_state"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "qF4C7Hf0uiVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_7b3f830c.py)\n",
        "\n"
      ],
      "metadata": {
        "execution": {},
        "id": "uB9c1FKKuiVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Perform policy evaluation { form-width: \"30%\" }\r\n",
        "# @markdown Here you can visualize the state value and action-value functions for the \"simple\" task.\r\n",
        "num_steps = 1e3\r\n",
        "\r\n",
        "# Create the environment\r\n",
        "grid = build_gridworld_task(task='simple')\r\n",
        "environment, environment_spec = setup_environment(grid)\r\n",
        "\r\n",
        "# Create the policy evaluation agent to evaluate a random policy.\r\n",
        "agent = PolicyEvalAgent(environment_spec, evaluated_policy=random_policy)\r\n",
        "\r\n",
        "# run experiment and get the value functions from agent\r\n",
        "returns = run_loop(environment=environment, agent=agent, num_steps=int(num_steps))\r\n",
        "\r\n",
        "# get the q-values\r\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4, ))\r\n",
        "\r\n",
        "# visualize value functions\r\n",
        "print('AFTER {} STEPS ...'.format(num_steps))\r\n",
        "plot_action_values(q, epsilon=1.)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "4vp7-n4IuiVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5.1: On-policy control: SARSA Agent\r\n",
        "In this section, we are focusing on control RL algorithms, which perform the **evaluation** and **improvement** of the policy synchronously. That is, the policy that is being evaluated improves as the agent is using it to interact with the environent.\r\n",
        "\r\n",
        "\r\n",
        "The first algorithm we are going to be looking at is SARSA. This is an **on-policy algorithm** -- i.e: the data collection is done by leveraging the policy we're trying to optimize. \r\n",
        "\r\n",
        "As discussed during lectures, a greedy policy with respect to a given $\\color{Green}Q$ fails to explore the environment as needed; we will use instead an $\\epsilon$-greedy policy with respect to $\\color{Green}Q$.\r\n",
        "\r\n",
        "### SARSA Algorithm\r\n",
        "\r\n",
        "**Input:**\r\n",
        "- $\\epsilon \\in (0, 1)$ the probability of taking a random action , and\r\n",
        "- $\\alpha > 0$ the step size, also known as learning rate.\r\n",
        "\r\n",
        "**Initialize:** $\\color{green}Q(\\color{red}{s}, \\color{blue}{a})$ for all $\\color{red}{s}$ ∈ $\\mathcal{\\color{red}S}$ and $\\color{blue}a$ ∈ $\\mathcal{\\color{blue}A}$\r\n",
        "\r\n",
        "**Loop forever:**\r\n",
        "\r\n",
        "1. Get $\\color{red}s \\gets{}$current (non-terminal) state\r\n",
        " \r\n",
        "2. Select $\\color{blue}a \\gets{} \\text{epsilon_greedy}(\\color{green}Q(\\color{red}s, \\cdot))$\r\n",
        " \r\n",
        "3. Step in the environment by passing the selected action $\\color{blue}a$\r\n",
        "\r\n",
        "4. Observe resulting reward $\\color{green}r$, discount $\\gamma$, and state $\\color{red}{s'}$\r\n",
        "\r\n",
        "5. Compute TD error: $\\Delta \\color{green}Q \\gets \r\n",
        "\\color{green}r + \\gamma \\color{green}Q(\\color{red}{s'}, \\color{blue}{a'}) − \\color{green}Q(\\color{red}s, \\color{blue}a)$, <br> where $\\color{blue}{a'} \\gets \\text{epsilon_greedy}(\\color{green}Q(\\color{red}{s'}, \\cdot))$\r\n",
        "\r\n",
        "5. Update $\\color{green}Q(\\color{red}s, \\color{blue}a) \\gets \\color{green}Q(\\color{red}s, \\color{blue}a) + \\alpha \\Delta \\color{green}Q$\r\n"
      ],
      "metadata": {
        "execution": {},
        "id": "3JI--AHruiVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Exercise 5.1: Implement $\\epsilon$-greedy\n",
        "Below you will find incomplete code for sampling from an $\\epsilon$-greedy policy, to be used later when we implement an agent that learns values according to the SARSA algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {},
        "id": "vdabEB-CuiVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def epsilon_greedy(\r\n",
        "    q_values_at_s: np.ndarray,  # Q-values in state s: Q(s, a).\r\n",
        "    epsilon: float = 0.1  # Probability of taking a random action.\r\n",
        "    ):\r\n",
        "  \"\"\"Return an epsilon-greedy action sample.\"\"\"\r\n",
        "  #################################################\r\n",
        "  # Fill in missing code below (...),\r\n",
        "  # then remove or comment the line below to test your implementation\r\n",
        "  raise NotImplementedError(\"Student exercise: complete epsilon greedy policy function\")\r\n",
        "  #################################################\r\n",
        "  # TODO generate a uniform random number and compare it to epsilon to decide if\r\n",
        "  # the action should be greedy or not\r\n",
        "  # HINT: Use np.random.random() to generate a random float from 0 to 1.\r\n",
        "  if ...:\r\n",
        "    #TODO Greedy: Pick action with the largest Q-value.\r\n",
        "    action = ...\r\n",
        "  else:\r\n",
        "    # Get the number of actions from the size of the given vector of Q-values.\r\n",
        "    num_actions = np.array(q_values_at_s).shape[-1]\r\n",
        "    # TODO else return a random action\r\n",
        "    # HINT: Use np.random.randint() to generate a random integer.\r\n",
        "    action = ...\r\n",
        "\r\n",
        "  return action"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "sXhHuVjouiVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_524ce08f.py)\n",
        "\n"
      ],
      "metadata": {
        "execution": {},
        "id": "Zr5xw6pFuiVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Sample action from $\\epsilon$-greedy { form-width: \"30%\" }\r\n",
        "# @markdown With $\\epsilon=0.5$, you should see that about half the time, you will get back the optimal\r\n",
        "# @markdown action 3, but half the time, it will be random.\r\n",
        "\r\n",
        "# Create fake q-values\r\n",
        "q_values = np.array([0, 0, 0, 1])\r\n",
        "\r\n",
        "# Set epsilon = 0.5\r\n",
        "epsilon = 0.5\r\n",
        "action = epsilon_greedy(q_values, epsilon=epsilon)\r\n",
        "print(action)"
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "3SCPgQZ8uiVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Exercise 5.2: Run your SARSA agent on the `obstacle` environment\n",
        "\n",
        "This environment is similar to the Cliff-walking example from [Sutton & Barto](http://incompleteideas.net/book/RLbook2018.pdf) and allows us to see the different policies learned by on-policy vs off-policy methods. Try varying the number of steps."
      ],
      "metadata": {
        "execution": {},
        "id": "VDSpoNxNuiVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class SarsaAgent(acme.Actor):\r\n",
        "\r\n",
        "  def __init__(self,\r\n",
        "               environment_spec: specs.EnvironmentSpec,\r\n",
        "               epsilon: float,\r\n",
        "               step_size: float = 0.1\r\n",
        "               ):\r\n",
        "\r\n",
        "    # Get number of states and actions from the environment spec.\r\n",
        "    self._num_states = environment_spec.observations.num_values\r\n",
        "    self._num_actions = environment_spec.actions.num_values\r\n",
        "\r\n",
        "    # Create the table of Q-values, all initialized at zero.\r\n",
        "    self._q = np.zeros((self._num_states, self._num_actions))\r\n",
        "\r\n",
        "    # Store algorithm hyper-parameters.\r\n",
        "    self._step_size = step_size\r\n",
        "    self._epsilon = epsilon\r\n",
        "\r\n",
        "    # Containers you may find useful.\r\n",
        "    self._state = None\r\n",
        "    self._action = None\r\n",
        "    self._next_state = None\r\n",
        "\r\n",
        "  @property\r\n",
        "  def q_values(self):\r\n",
        "    return self._q\r\n",
        "\r\n",
        "  def select_action(self, observation):\r\n",
        "    return epsilon_greedy(self._q[observation], self._epsilon)\r\n",
        "\r\n",
        "  def observe_first(self, timestep):\r\n",
        "    # Set current state.\r\n",
        "    self._state = timestep.observation\r\n",
        "\r\n",
        "  def observe(self, action, next_timestep):\r\n",
        "    # Unpacking the timestep to lighten notation.\r\n",
        "    s = self._state\r\n",
        "    a = action\r\n",
        "    r = next_timestep.reward\r\n",
        "    g = next_timestep.discount\r\n",
        "    next_s = next_timestep.observation\r\n",
        "    # Compute the action that would be taken from the next state.\r\n",
        "    next_a = self.select_action(next_s)\r\n",
        "    # Compute the on-policy Q-value update.\r\n",
        "    self._action = a\r\n",
        "    self._next_state = next_s\r\n",
        "    #################################################\r\n",
        "    # Fill in missing code below (...),\r\n",
        "    # then remove or comment the line below to test your implementation\r\n",
        "    raise NotImplementedError(\"Student exercise: complete the on-policy Q-value update\")\r\n",
        "    #################################################\r\n",
        "    # TODO complete the line below to compute the temporal difference error\r\n",
        "    # HINT: see step 5 in the pseudocode above.\r\n",
        "    self._td_error = ...\r\n",
        "\r\n",
        "  def update(self):\r\n",
        "    # Optional unpacking to lighten notation.\r\n",
        "    s = self._state\r\n",
        "    a = self._action\r\n",
        "    #################################################\r\n",
        "    # Fill in missing code below (...),\r\n",
        "    # then remove or comment the line below to test your implementation\r\n",
        "    raise NotImplementedError(\"Student exercise: complete value update\")\r\n",
        "    #################################################\r\n",
        "    # Update the Q-value table value at (s, a).\r\n",
        "    # TODO: Update the Q-value table value at (s, a).\r\n",
        "    # HINT: see step 6 in the pseudocode above, remember that alpha = step_size!\r\n",
        "    self._q[s, a] += ...\r\n",
        "    # Update the current state.\r\n",
        "    self._state = self._next_state"
      ],
      "outputs": [],
      "metadata": {
        "execution": {},
        "id": "8ZTr9vqkuiVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Acme agent"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\r\n",
        "## Appendix and further reading\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Books and lecture notes\r\n",
        "*   [Reinforcement Learning: an Introduction by Sutton & Barto](http://incompleteideas.net/book/RLbook2018.pdf)\r\n",
        "* [Algorithms for Reinforcement Learning by Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)\r\n",
        "\r\n",
        "Lectures and course \r\n",
        "*   [RL Course by David Silver](https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-)\r\n",
        "*   [Reinforcement Learning Course | UCL & DeepMind](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)\r\n",
        "*   [Emma Brunskill Stanford RL Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u)\r\n",
        "*   [RL Course on Coursera by Martha White & Adam White](https://www.coursera.org/specializations/reinforcement-learning)\r\n",
        "\r\n",
        "More practical:\r\n",
        "* [Spinning Up in Deep RL by Josh Achiam](https://spinningup.openai.com/en/latest/)\r\n",
        "*   [Acme white paper](https://arxiv.org/abs/2006.00979) & [Colab tutorial](https://github.com/deepmind/acme/blob/master/examples/tutorial.ipynb)\r\n",
        "\r\n",
        " <br>\r\n",
        "\r\n",
        "[Link to the tweet thread with resources recommended by the community](https://twitter.com/FeryalMP/status/1407272291579355136?s=20). \r\n",
        " \r\n",
        "<br>\r\n",
        "\r\n",
        "This Colab is based on the [EEML 2020 RL practical](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb) by Feryal Behbahani & Gheorghe Comanici. If you are interested in JAX you should try the colab. If you are interested in Tensorflow, there is also a version of the colab for the [MLSS 2020 RL Tutorial](https://github.com/Feryal/rl_mlss_2020) that you can try :)\r\n"
      ],
      "metadata": {
        "execution": {},
        "id": "RlpXST0BuiVz"
      }
    }
  ]
}